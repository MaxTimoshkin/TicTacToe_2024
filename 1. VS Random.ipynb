{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"jas5z2XbgpIK"},"outputs":[],"source":["import numpy as np\n","from collections import deque\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1724502632201,"user":{"displayName":"Максим Тимошкин","userId":"09734858170620036336"},"user_tz":-180},"id":"eGa4uhXjfHDA","outputId":"5b386bb4-b474-45b1-fda1-2254cb37e803"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{},"execution_count":2}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HkV_S73IfMyq"},"outputs":[],"source":["# Игра крестики-нолики\n","class TicTacToe:\n","    def __init__(self, player_1, player_2, board_size=3, win_size=3):\n","        self.players = {-1: player_1,\n","                         1: player_2}\n","\n","        self.wins = {player_1.name: 0,\n","                     player_2.name: 0}\n","\n","        self.board_size=board_size\n","        self.win_size = win_size\n","        self._kernel = self._create_kernel()\n","\n","\n","    # Создает ядро свертки для расчета побед\n","    def _create_kernel(self):\n","        kernel = np.zeros((2 * self.win_size + 2, self.win_size, self.win_size))\n","        for i in range(self.win_size):\n","            kernel[i, i, :] = np.ones(self.win_size)\n","        for i in range(self.win_size, 2 * self.win_size):\n","            kernel[i, :, i - self.win_size] = np.ones(self.win_size).T\n","        kernel[2 * self.win_size] = np.eye(self.win_size)\n","        kernel[2 * self.win_size + 1] = np.fliplr(np.eye(self.win_size))\n","        return kernel\n","\n","\n","    # Проверяет победы для состояний states, в кот. ходы были совершены игроками turns, turn={-1, 1}\n","    def _test_win(self, state, turn):\n","        rows, cols, w_size = *state.shape, self.win_size\n","        expanded_states = np.lib.stride_tricks.as_strided(\n","            state,\n","            shape=(rows - w_size + 1, cols - w_size + 1, w_size, w_size),\n","            strides=(*state.strides, *state.strides),\n","            writeable=False,\n","        )\n","        feature_map = np.einsum('xyij,sij->sxy', expanded_states, self._kernel)\n","        return -turn * (feature_map == turn * w_size).any().astype(int)\n","\n","\n","    # Проигрывание нескольких полных эпизодов\n","    def play(self, num_games=1, visualize=False):\n","        transitions = []\n","        for t in range(num_games):\n","            next_turn = turn = -1\n","            state = (np.zeros((self.board_size, self.board_size)), turn) # Начальное состояние игры. state = (state2d, turn)\n","            if visualize:\n","                self.visualize_state(state, turn)\n","            while(next_turn != 0):\n","                state_2d, turn = state\n","                current_player = self.players[turn]\n","                action = current_player.get_action(state)\n","                next_state_2d, next_turn, reward = self.play_turn(state, action)\n","                transitions.append((turn * state_2d, action, reward, -turn * next_state_2d, next_turn == 0))   #state, action, reward, new_state, done\n","                if visualize:\n","                    self.visualize_state((next_state_2d, next_turn), turn)\n","                if next_turn == 0:\n","                    if visualize:\n","                        if (reward == 0): print('Ничья!\\n')\n","                        else: print(f'Победа ({self.players[reward * turn].name})!\\n')\n","                    if reward != 0:\n","                        self.wins[self.players[reward * turn].name] += 1\n","                    self.players = {-1: self.players[1], 1: self.players[-1]}\n","                state = next_state_2d, next_turn\n","        return transitions\n","\n","\n","    # Выполнение хода и проверка на некорректный ход (проигрышь) / выигрыш / ничью\n","    def play_turn(self, state, action): # next_state2d, next_turn, reward\n","        state2d, turn = state\n","        next_state2d = state2d.copy()\n","\n","        # Проверка корректности хода\n","        if (state2d[(action)] != 0):\n","            return next_state2d, 0, -1        # Игрок проиграл (# next_turn == 0 => Игра окончена)\n","\n","        # Совершение хода\n","        next_state2d[action] = turn\n","\n","        # Проверка победы\n","        if self._test_win(next_state2d, turn):\n","            return next_state2d, 0, 1         # Текущий игрок побеждает (next_turn == 0 => Игра окончена)\n","\n","        # Проверка ничьи\n","        if (next_state2d != 0).all():\n","            return next_state2d, 0, 0         # Ничья (next_turn == 0 => Игра окончена)\n","\n","        # Инчае, ход следующего игрока\n","        return next_state2d, -turn, 0         # next_turn == -turn => Смена хода\n","\n","\n","    # Выводит на экран состояние игры после хода игрока\n","    @staticmethod\n","    def visualize_state(next_state, turn):\n","        next_state2d, next_turn = next_state\n","        print(f\"player {turn}'s turn:\")\n","        if (next_state2d == 0).all() and turn == 0:\n","            print(\"[invalid state]\\n\\n\")\n","        else:\n","            print(str(next_state2d)\n","                  .replace(\".\", \"\")\n","                  .replace(\"[[\", \"\")\n","                  .replace(\" [\", \"\")\n","                  .replace(\"]]\", \"\")\n","                  .replace(\"]\", \"\")\n","                  .replace(\"-0\", \" .\")\n","                  .replace(\"0\", \".\")\n","                  .replace(\"-1\", \" X\")\n","                  .replace(\"1\", \"O\")\n","            )\n","\n","\n","    @staticmethod\n","    def print_transitions(transitions):\n","        states, actions, rewards, next_states, dones = zip(*transitions)\n","        for i in np.arange(len(states)):\n","            print(\"\\033[31m{}.\".format(i + 1), '\\033[30m')\n","            TicTacToe.visualize_state((next_states[i], -1), 1)\n","            print('\\naction = ', actions[i] + np.array([1, 1]), end='\\n')\n","            print('reward = ', rewards[i], end='\\n')\n","            if (dones[i]): print('Игра окончена', end='\\n\\n')\n","            else: print('Игра продолжается', end='\\n\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o5rtWxp4hTVB"},"outputs":[],"source":["class Human:\n","    def __init__(self, name='Human'):\n","        self.name = name\n","\n","    def get_action(self, state):\n","        state2d, turn = state\n","        print('Введите ваш ход (Строка, столбец)')\n","        row, col = map(int, input().split())\n","        while (state2d[row - 1, col - 1] != 0):\n","            print('Клетка занята!')\n","            print('Введите ваш ход (Строка, столбец)')\n","            row, col = map(int, input().split())\n","        return row - 1, col - 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TERP9O1NMX0i"},"outputs":[],"source":["class Random:\n","    def __init__(self, name='Random'):\n","        self.name = name\n","\n","    def get_action(self, state):\n","        state2d, turn = state\n","        zero_idxs = np.argwhere(state2d == 0)\n","        return tuple(zero_idxs[np.random.randint(len(zero_idxs))])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gRBw9v0hmVIi"},"outputs":[],"source":["class DQNAgent(nn.Module):\n","    def __init__(self, epsilon=0, name='DQNAgent'):\n","        super().__init__()\n","\n","        self.name = name\n","        self.epsilon = epsilon\n","\n","        self.n_channels = 3\n","\n","        self.network = nn.Sequential(\n","            nn.Conv2d(self.n_channels, 32, kernel_size=(3, 3), padding='same'),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 128, kernel_size=(3, 3), padding='same'),\n","            nn.ReLU(),\n","            nn.Conv2d(128, 128, kernel_size=(3, 3), padding='same'),\n","            nn.ReLU(),\n","            nn.Conv2d(128, 1, kernel_size=(3, 3), padding='same')\n","        )\n","\n","    def forward(self, x):\n","        x = torch.stack([x == 1, x == -1, x == 0], axis=1).float()\n","        return self.network(x).squeeze(1)\n","\n","    def greedy_action(self, state, device=device):\n","        state2d, turn = state\n","        state_t = torch.FloatTensor(turn * state2d).unsqueeze(0).to(device)\n","        q_values = self.forward(state_t).squeeze(0).detach().cpu().numpy()\n","        # q_values[state2d != 0] = -float(\"Inf\") # Маскирование\n","        return np.unravel_index(q_values.argmax(), q_values.shape)\n","\n","    def random_action(self, state):\n","        state2d, turn = state\n","        zero_idxs = np.argwhere(state2d == 0)\n","        return tuple(zero_idxs[np.random.randint(len(zero_idxs))])\n","\n","    def get_action(self, state):\n","        if random.random() < self.epsilon:\n","            action = self.random_action(state)\n","        else:\n","            action = self.greedy_action(state)\n","        return action"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ni-Mh6jE_-Xd"},"outputs":[],"source":["class ReplayBuffer(object):\n","    def __init__(self, size):\n","        self._storage = deque(maxlen=size)\n","\n","    def __len__(self):\n","        return len(self._storage)\n","\n","    def add(self, transition):\n","        self._storage.append(transition)\n","\n","    def sample(self, batch_size):\n","        batch = random.sample(self._storage, batch_size)\n","        states, actions, rewards, next_states, dones = zip(*batch)\n","        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZWj7D_iuy3oT"},"outputs":[],"source":["seed = 42\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rRFnHMHaC7um"},"outputs":[],"source":["board_size = 5\n","win_size = 4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T_gPKyIB504V"},"outputs":[],"source":["# Гиперпараметры метода DQN\n","\n","batch_size = 128\n","total_steps = 30_000\n","\n","decay_steps = 18_000\n","init_epsilon = 1\n","final_epsilon = 0.02\n","\n","loss_freq = 400\n","refresh_target_network_freq = 500\n","\n","eval_freq = 1000\n","n_eval_games = 50\n","\n","max_grad_norm = 50\n","\n","gamma = 1.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AuZM4BiVn_8X"},"outputs":[],"source":["agent = DQNAgent(init_epsilon).to(device)\n","\n","target_network = DQNAgent(init_epsilon).to(device)\n","target_network.load_state_dict(agent.state_dict())\n","\n","optimizer = torch.optim.Adam(agent.parameters(), lr=1e-4)\n","exp_replay = ReplayBuffer(16_000)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1724502637665,"user":{"displayName":"Максим Тимошкин","userId":"09734858170620036336"},"user_tz":-180},"id":"kxFgb6XcdQ4V","outputId":"52a88f82-9094-406f-c9b5-3ded2a5c5330"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["186625"]},"metadata":{},"execution_count":12}],"source":["sum(p.numel() for p in agent.parameters() if p.requires_grad)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cKGcoDrQCzXi"},"outputs":[],"source":["# Возвращает temporal difference loss\n","def compute_td_loss(states, actions, rewards, next_states, dones,\n","                    agent, target_network, gamma=0.9, device=device):\n","\n","    states = torch.tensor(states, device=device, dtype=torch.float32)                # shape: [batch_size, state_dim]\n","    actions = torch.tensor(actions, device=device, dtype=torch.int64)                # shape: [batch_size]\n","    rewards = torch.tensor(rewards, device=device, dtype=torch.float32)              # shape: [batch_size]\n","    next_states = torch.tensor(next_states, device=device, dtype=torch.float32)      # shape: [batch_size, state_dim]\n","    dones = torch.tensor(dones, device=device, dtype=torch.int64)                    # shape: [batch_size]\n","\n","    predicted_qvalues = agent(states)                                                # shape: [batch_size, n_actions]\n","    predicted_next_qvalues = target_network(next_states)                             # shape: [batch_size, n_actions]\n","    predicted_qvalues_for_actions = predicted_qvalues[range(len(actions)), actions[:, 0], actions[:, 1]]  # shape: [batch_size]\n","    next_state_values = predicted_next_qvalues.view(dones.shape[0], -1).max(axis=1).values\n","    target_qvalues_for_actions = rewards - (1 - dones) * gamma * next_state_values\n","    return torch.mean((predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2)  #loss\n","\n","# Рассчитывает epsilon на текущем шаге step\n","def linear_decay(init_epsilon, final_epsilon, step, decay_steps):\n","    return max(init_epsilon - step * (init_epsilon - final_epsilon) / decay_steps, final_epsilon)"]},{"cell_type":"markdown","metadata":{"id":"e73vABV6cKH2"},"source":["# Обучение"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28464,"status":"ok","timestamp":1724502666124,"user":{"displayName":"Максим Тимошкин","userId":"09734858170620036336"},"user_tz":-180},"id":"IigXAGYNdoU7","outputId":"fd94e22c-bd9d-4fb6-d130-20a859be9f11"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GVtRXPFnZ1ug"},"outputs":[],"source":["game = TicTacToe(agent, Random(), board_size=board_size, win_size=win_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D4mXgvfVsN-2"},"outputs":[],"source":["PATH = f'/content/drive/MyDrive/TicTacToe_2024/TicTacToe'\n","\n","loss = None\n","\n","loss_values = []\n","reward_values = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SSNH2zNO3fVB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718137683147,"user_tz":-180,"elapsed":2105420,"user":{"displayName":"Олег Дуров","userId":"00246120506671741909"}},"outputId":"76f84cd3-b479-4dcd-a677-b04147f20266"},"outputs":[{"output_type":"stream","name":"stdout","text":["t = 0    \t reward = -0.96\n","\n","t = 400  \t loss = 0.003499798011034727\t eps = 0.9782\n","t = 800  \t loss = 0.006664647720754147\t eps = 0.9564\n","t = 1000 \t reward = 0.56\n","\n","t = 1200 \t loss = 0.010979915037751198\t eps = 0.9347\n","t = 1600 \t loss = 0.019739609211683273\t eps = 0.9129\n","t = 2000 \t loss = 0.007432692684233189\t eps = 0.8911\n","t = 2000 \t reward = 0.8\n","\n","t = 2400 \t loss = 0.00967826135456562\t eps = 0.8693\n","t = 2800 \t loss = 0.007575429044663906\t eps = 0.8476\n","t = 3000 \t reward = 0.84\n","\n","t = 3200 \t loss = 0.014437269419431686\t eps = 0.8258\n","t = 3600 \t loss = 0.010891581885516644\t eps = 0.804\n","t = 4000 \t loss = 0.00514253880828619\t eps = 0.7822\n","t = 4000 \t reward = 0.88\n","\n","t = 4400 \t loss = 0.005044175777584314\t eps = 0.7604\n","t = 4800 \t loss = 0.003672633785754442\t eps = 0.7387\n","t = 5000 \t reward = 0.88\n","\n","t = 5200 \t loss = 0.010673675686120987\t eps = 0.7169\n","t = 5600 \t loss = 0.009115155786275864\t eps = 0.6951\n","t = 6000 \t loss = 0.00799708440899849\t eps = 0.6733\n","t = 6000 \t reward = 0.84\n","\n","t = 6400 \t loss = 0.006096067372709513\t eps = 0.6516\n","t = 6800 \t loss = 0.0018061732407659292\t eps = 0.6298\n","t = 7000 \t reward = 0.96\n","\n","t = 7200 \t loss = 0.0023200586438179016\t eps = 0.608\n","t = 7600 \t loss = 0.0034398427233099937\t eps = 0.5862\n","t = 8000 \t loss = 0.0026142303831875324\t eps = 0.5644\n","t = 8000 \t reward = 0.92\n","\n","t = 8400 \t loss = 0.003182042622938752\t eps = 0.5427\n","t = 8800 \t loss = 0.0029819891788065434\t eps = 0.5209\n","t = 9000 \t reward = 0.96\n","\n","t = 9200 \t loss = 0.004776290617883205\t eps = 0.4991\n","t = 9600 \t loss = 0.005611097440123558\t eps = 0.4773\n","t = 10000\t loss = 0.0026148618198931217\t eps = 0.4556\n","t = 10000\t reward = 0.96\n","\n","t = 10400\t loss = 0.002197456546127796\t eps = 0.4338\n","t = 10800\t loss = 0.002392871305346489\t eps = 0.412\n","t = 11000\t reward = 0.96\n","\n","t = 11200\t loss = 0.0028981962241232395\t eps = 0.3902\n","t = 11600\t loss = 0.0028445736970752478\t eps = 0.3684\n","t = 12000\t loss = 0.0037888321094214916\t eps = 0.3467\n","t = 12000\t reward = 1.0\n","\n","t = 12400\t loss = 0.00209191907197237\t eps = 0.3249\n","t = 12800\t loss = 0.0015348161105066538\t eps = 0.3031\n","t = 13000\t reward = 1.0\n","\n","t = 13200\t loss = 0.004031783435493708\t eps = 0.2813\n","t = 13600\t loss = 0.0022484755609184504\t eps = 0.2596\n","t = 14000\t loss = 0.0014658409636467695\t eps = 0.2378\n","t = 14000\t reward = 0.96\n","\n","t = 14400\t loss = 0.0015867538750171661\t eps = 0.216\n","t = 14800\t loss = 0.0008685238426551223\t eps = 0.1942\n","t = 15000\t reward = 1.0\n","\n","t = 15200\t loss = 0.002552805934101343\t eps = 0.1724\n","t = 15600\t loss = 0.0011130820494145155\t eps = 0.1507\n","t = 16000\t loss = 0.0005975405219942331\t eps = 0.1289\n","t = 16000\t reward = 1.0\n","\n","t = 16400\t loss = 0.0006904187612235546\t eps = 0.1071\n","t = 16800\t loss = 0.00039821816608309746\t eps = 0.0853\n","t = 17000\t reward = 0.96\n","\n","t = 17200\t loss = 0.0005961601855233312\t eps = 0.0636\n","t = 17600\t loss = 0.00021198668400757015\t eps = 0.0418\n","t = 18000\t loss = 0.0007299876306205988\t eps = 0.02\n","t = 18000\t reward = 1.0\n","\n","t = 18400\t loss = 0.00018271521548740566\t eps = 0.02\n","t = 18800\t loss = 0.00017305643996223807\t eps = 0.02\n","t = 19000\t reward = 1.0\n","\n","t = 19200\t loss = 0.00038512097671628\t eps = 0.02\n","t = 19600\t loss = 0.00535542331635952\t eps = 0.02\n","t = 20000\t loss = 0.0017804033122956753\t eps = 0.02\n","t = 20000\t reward = 1.0\n","\n","t = 20400\t loss = 0.00013408424274530262\t eps = 0.02\n","t = 20800\t loss = 0.0003559818142093718\t eps = 0.02\n","t = 21000\t reward = 0.88\n","\n","t = 21200\t loss = 3.196878969902173e-05\t eps = 0.02\n","t = 21600\t loss = 0.002215274376794696\t eps = 0.02\n","t = 22000\t loss = 0.00017671554815024137\t eps = 0.02\n","t = 22000\t reward = 1.0\n","\n","t = 22400\t loss = 0.0001624987635295838\t eps = 0.02\n","t = 22800\t loss = 0.00024465040769428015\t eps = 0.02\n","t = 23000\t reward = 0.96\n","\n","t = 23200\t loss = 5.807633715448901e-05\t eps = 0.02\n","t = 23600\t loss = 0.00016409614181611687\t eps = 0.02\n","t = 24000\t loss = 8.95629491424188e-05\t eps = 0.02\n","t = 24000\t reward = 0.96\n","\n","t = 24400\t loss = 0.00010107468551723287\t eps = 0.02\n","t = 24800\t loss = 0.0002662724582478404\t eps = 0.02\n","t = 25000\t reward = 1.0\n","\n","t = 25200\t loss = 0.0001299362484132871\t eps = 0.02\n","t = 25600\t loss = 0.0003944002673961222\t eps = 0.02\n","t = 26000\t loss = 9.058135037776083e-05\t eps = 0.02\n","t = 26000\t reward = 1.0\n","\n","t = 26400\t loss = 0.00026507111033424735\t eps = 0.02\n","t = 26800\t loss = 3.362333882250823e-05\t eps = 0.02\n","t = 27000\t reward = 1.0\n","\n","t = 27200\t loss = 0.00021164858480915427\t eps = 0.02\n","t = 27600\t loss = 0.00013190647587180138\t eps = 0.02\n","t = 28000\t loss = 6.045131158316508e-05\t eps = 0.02\n","t = 28000\t reward = 1.0\n","\n","t = 28400\t loss = 0.0002217392175225541\t eps = 0.02\n","t = 28800\t loss = 0.000574969919398427\t eps = 0.02\n","t = 29000\t reward = 1.0\n","\n","t = 29200\t loss = 0.0005457442020997405\t eps = 0.02\n","t = 29600\t loss = 0.0004022180219180882\t eps = 0.02\n"]}],"source":["with open('out.txt', 'w') as f:\n","  for t in range(total_steps):\n","    print(f't = {t}. Ход {game.players[-1].name}', file=f)\n","\n","    state = (np.zeros((board_size, board_size)), -1) # Начальное состояние игры. state = (state_2d, turn)\n","    turn = next_turn = -1\n","\n","    while(next_turn != 0):\n","        current_player = game.players[turn]\n","        if current_player.name == 'DQNAgent':\n","            agent.epsilon = linear_decay(init_epsilon, final_epsilon, t, decay_steps)\n","\n","        action = agent.get_action(state)\n","        print(action, file=f)\n","        next_state_2d, next_turn, reward = game.play_turn(state, action)\n","\n","        if next_turn == 0:\n","            if (reward == 0): print('Ничья!\\n', file=f)\n","            else: print(f'Победа ({game.players[reward * turn].name})!\\n', file=f)\n","            game.players = {-1: game.players[1], 1: game.players[-1]}\n","\n","        state_2d, turn = state\n","        exp_replay.add((turn * state_2d, action, reward, next_turn * next_state_2d, next_turn == 0)) #state, action, reward, new_state, done\n","\n","        # Обучение на минибатче\n","        if len(exp_replay) >= batch_size:\n","            states, actions, rewards, next_states, dones = exp_replay.sample(batch_size)\n","            loss = compute_td_loss(states, actions, rewards, next_states, dones, agent, target_network)\n","            loss.backward()\n","            grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        state = next_state_2d, next_turn\n","\n","    # Каждые refresh_target_network_freq обновляются веса target сети\n","    if t % refresh_target_network_freq == 0:\n","        target_network.load_state_dict(agent.state_dict())\n","\n","    # Вывод лосса с заданной частотой\n","    if t % loss_freq == 0 and t > 0:\n","        loss_values.append(loss)\n","        print(f\"t = {str(t):5}\\t loss = {loss}\\t eps = {round(agent.epsilon, 4)}\")\n","\n","    # Вывод награды с заданной частотой\n","    if t % eval_freq == 0:\n","        agent.epsilon = 0\n","        eval_game = TicTacToe(agent, Random(), board_size=board_size, win_size=win_size)\n","        eval_game.play(n_eval_games)\n","        mean_reward = (eval_game.wins['DQNAgent'] - eval_game.wins['Random']) / n_eval_games\n","        reward_values.append(mean_reward)\n","        print(f\"t = {str(t):5}\\t reward = {round(mean_reward, 4)}\\n\")\n","\n","        torch.save(agent.state_dict(), PATH + f'model_{t}')\n","        torch.save(optimizer.state_dict(), PATH + f'opt_{t}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"66ac1083-be59-487d-f80f-d2d5c72ac2e6","id":"AW-TWC9sCkIO"},"outputs":[{"output_type":"stream","name":"stdout","text":["t = 30000\t loss = 0.00037415343103930354\t eps = 0.02\n","t = 30000\t reward = 1.0\n","\n","t = 30400\t loss = 0.000154425113578327\t eps = 0.02\n","t = 30800\t loss = 9.560085163684562e-05\t eps = 0.02\n","t = 31000\t reward = 0.88\n","\n","t = 31200\t loss = 6.0891059547429904e-05\t eps = 0.02\n","t = 31600\t loss = 0.0005621250020340085\t eps = 0.02\n","t = 32000\t loss = 0.0004982273676432669\t eps = 0.02\n","t = 32000\t reward = 0.94\n","\n","t = 32400\t loss = 0.00011836771591333672\t eps = 0.02\n","t = 32800\t loss = 0.0003899220610037446\t eps = 0.02\n","t = 33000\t reward = 1.0\n","\n","t = 33200\t loss = 5.6718930864008144e-05\t eps = 0.02\n","t = 33600\t loss = 0.00036711295251734555\t eps = 0.02\n","t = 34000\t loss = 0.00025227334117516875\t eps = 0.02\n","t = 34000\t reward = 1.0\n","\n","t = 34400\t loss = 0.00024430619669146836\t eps = 0.02\n","t = 34800\t loss = 0.00016894566942937672\t eps = 0.02\n","t = 35000\t reward = 0.96\n","\n","t = 35200\t loss = 0.004832028411328793\t eps = 0.02\n","t = 35600\t loss = 0.0016375058330595493\t eps = 0.02\n","t = 36000\t loss = 4.0300168620888144e-05\t eps = 0.02\n","t = 36000\t reward = 1.0\n","\n","t = 36400\t loss = 0.00011717573943315074\t eps = 0.02\n","t = 36800\t loss = 0.00138541916385293\t eps = 0.02\n","t = 37000\t reward = 1.0\n","\n","t = 37200\t loss = 0.0006246676784940064\t eps = 0.02\n","t = 37600\t loss = 0.0011069660540670156\t eps = 0.02\n","t = 38000\t loss = 6.864616443635896e-05\t eps = 0.02\n","t = 38000\t reward = 0.92\n","\n","t = 38400\t loss = 6.22429943177849e-05\t eps = 0.02\n","t = 38800\t loss = 5.915982910664752e-05\t eps = 0.02\n","t = 39000\t reward = 0.96\n","\n","t = 39200\t loss = 0.00015147333033382893\t eps = 0.02\n","t = 39600\t loss = 0.000366813299478963\t eps = 0.02\n","t = 40000\t loss = 0.00011732740676961839\t eps = 0.02\n","t = 40000\t reward = 1.0\n","\n","t = 40400\t loss = 5.516758392332122e-05\t eps = 0.02\n","t = 40800\t loss = 0.000799995323177427\t eps = 0.02\n","t = 41000\t reward = 1.0\n","\n","t = 41200\t loss = 0.0013740194262936711\t eps = 0.02\n","t = 41600\t loss = 0.0007420176407322288\t eps = 0.02\n","t = 42000\t loss = 5.602843884844333e-05\t eps = 0.02\n","t = 42000\t reward = 0.96\n","\n","t = 42400\t loss = 0.0002055173390544951\t eps = 0.02\n","t = 42800\t loss = 0.00591885345056653\t eps = 0.02\n","t = 43000\t reward = 0.92\n","\n","t = 43200\t loss = 0.00010515603935346007\t eps = 0.02\n","t = 43600\t loss = 0.00029413579613901675\t eps = 0.02\n","t = 44000\t loss = 0.0008561087888665497\t eps = 0.02\n","t = 44000\t reward = 0.92\n","\n","t = 44400\t loss = 0.00554077560082078\t eps = 0.02\n","t = 44800\t loss = 0.00013090588618069887\t eps = 0.02\n","t = 45000\t reward = 1.0\n","\n","t = 45200\t loss = 0.0002546609321143478\t eps = 0.02\n","t = 45600\t loss = 0.00025452510453760624\t eps = 0.02\n","t = 46000\t loss = 0.00048033578786998987\t eps = 0.02\n","t = 46000\t reward = 1.0\n","\n","t = 46400\t loss = 0.00013594437041319907\t eps = 0.02\n","t = 46800\t loss = 0.00012942953617312014\t eps = 0.02\n","t = 47000\t reward = 0.92\n","\n","t = 47200\t loss = 7.074714812915772e-05\t eps = 0.02\n","t = 47600\t loss = 0.0003213076270185411\t eps = 0.02\n","t = 48000\t loss = 0.00027325423434376717\t eps = 0.02\n","t = 48000\t reward = 1.0\n","\n","t = 48400\t loss = 0.00018653861479833722\t eps = 0.02\n","t = 48800\t loss = 0.00018438018742017448\t eps = 0.02\n","t = 49000\t reward = 1.0\n","\n","t = 49200\t loss = 8.23330192361027e-05\t eps = 0.02\n","t = 49600\t loss = 0.0004162202822044492\t eps = 0.02\n"]}],"source":["with open('out2.txt', 'w') as f:\n","  for t in range(30_000, 60_000):\n","    print(f't = {t}. Ход {game.players[-1].name}', file=f)\n","\n","    state = (np.zeros((board_size, board_size)), -1) # Начальное состояние игры. state = (state_2d, turn)\n","    turn = next_turn = -1\n","\n","    while(next_turn != 0):\n","        current_player = game.players[turn]\n","        if current_player.name == 'DQNAgent':\n","            agent.epsilon = linear_decay(init_epsilon, final_epsilon, t, decay_steps)\n","\n","        action = agent.get_action(state)\n","        print(action, file=f)\n","        next_state_2d, next_turn, reward = game.play_turn(state, action)\n","\n","        if next_turn == 0:\n","            if (reward == 0): print('Ничья!\\n', file=f)\n","            else: print(f'Победа ({game.players[reward * turn].name})!\\n', file=f)\n","            game.players = {-1: game.players[1], 1: game.players[-1]}\n","\n","        state_2d, turn = state\n","        exp_replay.add((turn * state_2d, action, reward, next_turn * next_state_2d, next_turn == 0)) #state, action, reward, new_state, done\n","\n","        # Обучение на минибатче\n","        if len(exp_replay) >= batch_size:\n","            states, actions, rewards, next_states, dones = exp_replay.sample(batch_size)\n","            loss = compute_td_loss(states, actions, rewards, next_states, dones, agent, target_network)\n","            loss.backward()\n","            grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        state = next_state_2d, next_turn\n","\n","    # Каждые refresh_target_network_freq обновляются веса target сети\n","    if t % refresh_target_network_freq == 0:\n","        target_network.load_state_dict(agent.state_dict())\n","\n","    # Вывод лосса с заданной частотой\n","    if t % loss_freq == 0 and t > 0:\n","        loss_values.append(loss)\n","        print(f\"t = {str(t):5}\\t loss = {loss}\\t eps = {round(agent.epsilon, 4)}\")\n","\n","    # Вывод награды с заданной частотой\n","    if t % eval_freq == 0:\n","        agent.epsilon = 0\n","        eval_game = TicTacToe(agent, Random(), board_size=board_size, win_size=win_size)\n","        eval_game.play(n_eval_games)\n","        mean_reward = (eval_game.wins['DQNAgent'] - eval_game.wins['Random']) / n_eval_games\n","        reward_values.append(mean_reward)\n","        print(f\"t = {str(t):5}\\t reward = {round(mean_reward, 4)}\\n\")\n","\n","        torch.save(agent.state_dict(), PATH + f'model_{t}')\n","        torch.save(optimizer.state_dict(), PATH + f'opt_{t}')"]},{"cell_type":"code","source":["state2d = torch.tensor(np.array(\n","    [[[0, 0, 0, 0, 0],\n","      [0, 0, 0, 0, 0],\n","      [0, 0, 0, 0, 0],\n","      [0, 0, 0, 0, 0],\n","      [0, 0, 0, 0, 0]]]\n",")).to(device)\n","\n","q_values = agent(state2d).squeeze(0).detach().cpu().numpy()\n","        # q_values[state2d != 0] = -float(\"Inf\") # Маскирование\n","np.unravel_index(q_values.argmax(), q_values.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aKjltLQC1-vk","executionInfo":{"status":"ok","timestamp":1718141283582,"user_tz":-180,"elapsed":342,"user":{"displayName":"Олег Дуров","userId":"00246120506671741909"}},"outputId":"31df8906-27ea-4398-bdf0-eab5d45b0c89"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2, 2)"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["q_values"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dTKnN9UG4TJN","executionInfo":{"status":"ok","timestamp":1718141286326,"user_tz":-180,"elapsed":354,"user":{"displayName":"Олег Дуров","userId":"00246120506671741909"}},"outputId":"ce1a07ca-2dbf-4347-f9a8-17521bc2e53c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-0.08061364, -0.05459422, -0.02210075, -0.04188852, -0.0907537 ],\n","       [-0.01836986,  0.00618161,  0.08391692,  0.05746449, -0.09141948],\n","       [-0.05466395,  0.02869976,  0.10814499,  0.08277152, -0.11566573],\n","       [-0.03168222,  0.08481603,  0.04625217,  0.06391506, -0.1531809 ],\n","       [-0.07397533, -0.01455764,  0.02738114, -0.03418991, -0.01185933]],\n","      dtype=float32)"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["agent.load_state_dict(torch.load(PATH + 'model_14000', map_location=torch.device('cpu')))\n","#agent.load_state_dict(torch.load(PATH + 'model_50000'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ifL1WQM4E03","executionInfo":{"status":"ok","timestamp":1718142123032,"user_tz":-180,"elapsed":372,"user":{"displayName":"Олег Дуров","userId":"00246120506671741909"}},"outputId":"6c04be49-24b3-4108-f3d7-1d2c3a936e39"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_bgzNN9tsPVN","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e8f359b1-bb44-42e6-9048-1313ab8c14dc"},"outputs":[{"output_type":"stream","name":"stdout","text":["player -1's turn:\n",". . . . .\n",". . . . .\n",". . . . .\n",". . . . .\n",". . . . .\n","player -1's turn:\n"," .  .  .  .  .\n"," .  .  .  .  .\n"," .  .  X  .  .\n"," .  .  .  .  .\n"," .  .  .  .  .\n","Введите ваш ход (Строка, столбец)\n","2 2\n","player 1's turn:\n"," .  .  .  .  .\n"," .  O  .  .  .\n"," .  .  X  .  .\n"," .  .  .  .  .\n"," .  .  .  .  .\n","player -1's turn:\n"," .  .  .  .  .\n"," .  O  X  .  .\n"," .  .  X  .  .\n"," .  .  .  .  .\n"," .  .  .  .  .\n","Введите ваш ход (Строка, столбец)\n","3 4\n","player 1's turn:\n"," .  .  .  .  .\n"," .  O  X  .  .\n"," .  .  X  O  .\n"," .  .  .  .  .\n"," .  .  .  .  .\n","player -1's turn:\n"," .  .  .  .  .\n"," .  O  X  .  .\n"," .  .  X  O  .\n"," .  .  X  .  .\n"," .  .  .  .  .\n","Введите ваш ход (Строка, столбец)\n","5 3\n","player 1's turn:\n"," .  .  .  .  .\n"," .  O  X  .  .\n"," .  .  X  O  .\n"," .  .  X  .  .\n"," .  .  O  .  .\n","player -1's turn:\n"," .  .  X  .  .\n"," .  O  X  .  .\n"," .  .  X  O  .\n"," .  .  X  .  .\n"," .  .  O  .  .\n","Победа (DQNAgent)!\n","\n","player -1's turn:\n",". . . . .\n",". . . . .\n",". . . . .\n",". . . . .\n",". . . . .\n","Введите ваш ход (Строка, столбец)\n","1 1\n","player -1's turn:\n"," X  .  .  .  .\n"," .  .  .  .  .\n"," .  .  .  .  .\n"," .  .  .  .  .\n"," .  .  .  .  .\n","player 1's turn:\n"," X  .  .  .  .\n"," .  .  .  .  .\n"," .  .  O  .  .\n"," .  .  .  .  .\n"," .  .  .  .  .\n","Введите ваш ход (Строка, столбец)\n","1 2\n","player -1's turn:\n"," X  X  .  .  .\n"," .  .  .  .  .\n"," .  .  O  .  .\n"," .  .  .  .  .\n"," .  .  .  .  .\n","player 1's turn:\n"," X  X  .  .  .\n"," .  .  .  O  .\n"," .  .  O  .  .\n"," .  .  .  .  .\n"," .  .  .  .  .\n","Введите ваш ход (Строка, столбец)\n","1 3\n","player -1's turn:\n"," X  X  X  .  .\n"," .  .  .  O  .\n"," .  .  O  .  .\n"," .  .  .  .  .\n"," .  .  .  .  .\n","player 1's turn:\n"," X  X  X  O  .\n"," .  .  .  O  .\n"," .  .  O  .  .\n"," .  .  .  .  .\n"," .  .  .  .  .\n","Введите ваш ход (Строка, столбец)\n","4 2\n","player -1's turn:\n"," X  X  X  O  .\n"," .  .  .  O  .\n"," .  .  O  .  .\n"," .  X  .  .  .\n"," .  .  .  .  .\n","player 1's turn:\n"," X  X  X  O  .\n"," .  .  .  O  .\n"," .  .  O  .  O\n"," .  X  .  .  .\n"," .  .  .  .  .\n","Введите ваш ход (Строка, столбец)\n","3 4\n","player -1's turn:\n"," X  X  X  O  .\n"," .  .  .  O  .\n"," .  .  O  X  O\n"," .  X  .  .  .\n"," .  .  .  .  .\n","player 1's turn:\n"," X  X  X  O  .\n"," .  .  .  O  .\n"," .  .  O  X  O\n"," .  X  .  .  .\n"," .  .  .  .  O\n","Введите ваш ход (Строка, столбец)\n"]}],"source":["agent.epsilon = 0\n","test_game = TicTacToe(agent, Human(), board_size=board_size, win_size=win_size)\n","test_game.play(5, True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N0rBFzDECyzr"},"outputs":[],"source":["1#Соранение списки rewards & losses\n","\n","with open(f'{PATH}data.pickle', 'wb') as f:\n","   pickle.dump((reward_values, loss_values), f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G_Kt7bpZ0LfZ"},"outputs":[],"source":["torch.save(agent.state_dict(), PATH + f'30000_model')\n","torch.save(optimizer.state_dict(), PATH + f'30000_opt')"]},{"cell_type":"code","source":[],"metadata":{"id":"pjxCPSP9Jm13"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}