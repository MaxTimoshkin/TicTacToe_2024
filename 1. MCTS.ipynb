{"cells":[{"cell_type":"markdown","source":["#–ò–≥—Ä–∞"],"metadata":{"id":"4OvXRohFxmhu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ojG25S68SqXm"},"outputs":[],"source":["import numpy as np\n","import math\n","import time\n","\n","#import pyspiel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LFRJloJeF-bx"},"outputs":[],"source":["# –°–æ—Å—Ç–æ—è–Ω–∏–µ –∏–≥—Ä—ã –∫—Ä–µ—Å—Ç–∏–∫–∏-–Ω–æ–ª–∏–∫–∏\n","class State_TicTacToe:\n","    def __init__(self, board_size=3, win_size=3):\n","        self.board_size=board_size\n","        self.win_size = win_size\n","\n","        self._kernel = self._create_kernel()\n","        self.restart()\n","\n","    def restart(self):\n","        self.state = np.zeros((self.board_size, self.board_size))\n","        self.turn = -1\n","        self.rewards = {-1: 0, +1: 0}\n","        self.is_done = False\n","\n","    # –°–æ–∑–¥–∞–µ—Ç —è–¥—Ä–æ —Å–≤–µ—Ä—Ç–∫–∏ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –ø–æ–±–µ–¥\n","    def _create_kernel(self):\n","        kernel = np.zeros((2 * self.win_size + 2, self.win_size, self.win_size))\n","        for i in range(self.win_size):\n","            kernel[i, i, :] = np.ones(self.win_size)\n","        for i in range(self.win_size, 2 * self.win_size):\n","            kernel[i, :, i - self.win_size] = np.ones(self.win_size).T\n","        kernel[2 * self.win_size] = np.eye(self.win_size)\n","        kernel[2 * self.win_size + 1] = np.fliplr(np.eye(self.win_size))\n","        return kernel\n","\n","\n","    # –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–±–µ–¥—ã –¥–ª—è —Å–æ—Å—Ç–æ—è–Ω–∏–π states, –≤ –∫–æ—Ç. —Ö–æ–¥—ã –±—ã–ª–∏ —Å–æ–≤–µ—Ä—à–µ–Ω—ã –∏–≥—Ä–æ–∫–∞–º–∏ turns, turn={-1, 1}\n","    def _test_win(self):\n","        rows, cols, w_size = *self.state.shape, self.win_size\n","        expanded_states = np.lib.stride_tricks.as_strided(\n","            self.state,\n","            shape=(rows - w_size + 1, cols - w_size + 1, w_size, w_size),\n","            strides=(*self.state.strides, *self.state.strides),\n","            writeable=False,\n","        )\n","        feature_map = np.einsum('xyij,sij->sxy', expanded_states, self._kernel)\n","        return -self.turn * (feature_map == self.turn * w_size).any().astype(int)\n","\n","\n","    # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ö–æ–¥–∞ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ö–æ–¥ (–ø—Ä–æ–∏–≥—Ä—ã—à—å) / –≤—ã–∏–≥—Ä—ã—à / –Ω–∏—á—å—é\n","    def apply_action(self, action):\n","        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ —Ö–æ–¥–∞\n","        if (self.state[(action)] != 0):\n","            self.rewards = {self.turn: -1, -self.turn: +1}\n","            self.is_done = True\n","            self.turn = 0\n","            return\n","\n","        # –°–æ–≤–µ—Ä—à–µ–Ω–∏–µ —Ö–æ–¥–∞\n","        self.state[action] = self.turn\n","\n","        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–±–µ–¥—ã\n","        if self._test_win():\n","            self.rewards = {self.turn: +1, -self.turn: -1}\n","            self.is_done = True\n","            self.turn = 0\n","            return\n","\n","        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∏—á—å–∏\n","        if (self.state != 0).all():\n","            self.is_done = True\n","            self.turn = 0\n","            return\n","\n","        # –ò–Ω–∞—á–µ, —Å–º–µ–Ω–∏—Ç—å —Ö–æ–¥\n","        self.turn = -self.turn\n","\n","    # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True, –µ—Å–ª–∏ –∏–≥—Ä–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞\n","    def is_terminal(self):\n","        return self.is_done\n","\n","    # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–≥—Ä—ã (–Ω–∞–≥—Ä–∞–¥—ã) –∫–∞–∂–¥–æ–≥–æ –∏–≥—Ä–æ–∫–∞\n","    def returns(self):\n","        return np.array([self.rewards[-1], self.rewards[+1]])\n","\n","    # –°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ø–∏—é —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è\n","    def clone(self):\n","        cloned = State_TicTacToe(self.board_size, self.win_size)\n","        cloned.state = np.copy(self.state)\n","        cloned.turn = self.turn\n","        cloned.rewards = dict(self.rewards)\n","        cloned.is_done = self.is_done\n","        return cloned\n","\n","    # –í—ã–≤–æ–¥–∏—Ç –Ω–∞ —ç–∫—Ä–∞–Ω —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏–≥—Ä—ã\n","    def visualize_state(self):\n","        print(f\"player {self.turn}'s turn:\")\n","        print(str(self.state)\n","              .replace(\".\", \"\")\n","              .replace(\"[[\", \"\")\n","              .replace(\" [\", \"\")\n","              .replace(\"]]\", \"\")\n","              .replace(\"]\", \"\")\n","              .replace(\"-0\", \" .\")\n","              .replace(\"0\", \".\")\n","              .replace(\"-1\", \" X\")\n","              .replace(\"1\", \"O\")\n","        )\n","\n","    # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –∏–ª–∏ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∏–≥—Ä–æ–∫–∞\n","    def legal_actions(self, player=None):\n","        return list(zip(*np.where(self.state == 0)))\n","\n","    # –í—ã–≤–æ–¥–∏—Ç —Ç–µ–∫—É—â–µ–≥–æ –∏–≥—Ä–æ–∫–∞\n","    def current_player(self):\n","        return 0 if self.turn == -1 else 1"]},{"cell_type":"markdown","metadata":{"id":"b4nREygnqbg9"},"source":["# MCTS"]},{"cell_type":"code","source":["import numpy as np\n","import math\n","import time"],"metadata":{"id":"Vl4oHePFewr7","executionInfo":{"status":"ok","timestamp":1720256880135,"user_tz":-180,"elapsed":480,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"pqpV4bckSySP","executionInfo":{"status":"ok","timestamp":1720256880643,"user_tz":-180,"elapsed":4,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"}}},"outputs":[],"source":["# –ü—Ä–æ—Å—Ç–æ–π evaluator, –¥–µ–ª–∞—é—â–∏–π —Å–ª—É—á–∞–π–Ω—ã–µ rollout'—ã\n","class RandomRolloutEvaluator(object):\n","    \"\"\"\n","    Evaluator –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ä–µ–¥–Ω–∏–π –≤—ã–∏–≥—Ä—ã—à, —Å–æ–≤–µ—Ä—à–∞—è —Å–ª—É—á–∞–π–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –∏–∑ –¥–∞–Ω–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è, –ø–æ–∫–∞ –∏–≥—Ä–∞ –Ω–µ –∑–∞–∫–æ–Ω—á–∏—Ç—Å—è.\n","    n_rollouts - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª—É—á–∞–π–Ω—ã—Ö rollout'–æ–≤.\n","    \"\"\"\n","\n","    def __init__(self, n_rollouts=1, random_state=None):\n","        self.n_rollouts = n_rollouts\n","        self._random_state = random_state or np.random.RandomState()\n","\n","    # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç V(s)\n","    def evaluate(self, state):\n","        result = 0\n","        for _ in range(self.n_rollouts):\n","            working_state = state.clone()\n","            while not working_state.is_terminal():\n","                actions = working_state.legal_actions()\n","                action = actions[self._random_state.choice(len(actions))]\n","                working_state.apply_action(action)\n","            result += np.array(working_state.returns())\n","        return result / self.n_rollouts\n","\n","    # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç ùúã(a|s)\n","    def prior(self, state):\n","        legal_actions = state.legal_actions()\n","        return [(action, 1.0 / len(legal_actions)) for action in legal_actions]"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"EMy9NwEyS4j_","executionInfo":{"status":"ok","timestamp":1720256880644,"user_tz":-180,"elapsed":4,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"}}},"outputs":[],"source":["# –í–µ—Ä—à–∏–Ω–∞ –¥–µ—Ä–µ–≤–∞ –ø–æ–∏—Å–∫–∞\n","class SearchNode(object):\n","    \"\"\"\n","    A SearchNode represents a state and possible continuations from it. Each child\n","    represents a possible action, and the expected result from doing so.\n","\n","    Attributes:\n","        action: –î–µ–π—Å—Ç–≤–∏–µ a –∏–∑ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ —É–∑–ª–∞ s.\n","        player: –ò–≥—Ä–æ–∫, —Å–æ–≤–µ—Ä—à–∏–≤—à–∏–π –¥–µ–π—Å—Ç–≤–∏–µ.\n","        prior: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤—ã–±–æ—Ä–∞ –¥–µ–π—Å—Ç–≤–∏—è P(s,a).\n","        explore_count: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã–±–æ—Ä–∞ –¥–∞–Ω–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è N(s,a).\n","        total_reward: –°—É–º–º–∞ –Ω–∞–≥—Ä–∞–¥ –æ—Ç —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ —É–∑–ª–∞ W(s,a).\n","            –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ Q(s,a) = W(s,a) / N(s,a).\n","        outcome: The rewards for all players if this is a terminal node or the\n","            subtree has been proven, otherwise None.\n","        children: A list of SearchNodes representing the possible actions from this\n","            node, along with their expected rewards.\n","    \"\"\"\n","    __slots__ = [\"action\", \"player\", \"prior\", \"explore_count\", \"total_reward\", \"outcome\", \"children\"]\n","\n","    def __init__(self, action, player, prior):\n","        self.action = action      # a\n","        self.prior = prior        # P(s,a)\n","        self.explore_count = 0    # N(s,a)\n","        self.total_reward = 0.0   # W(s,a)\n","\n","        self.player = player\n","        self.outcome = None\n","        self.children = []\n","\n","    # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç UCT –¥–æ—á–µ—Ä–Ω–µ–≥–æ —É–∑–ª–∞\n","    def uct_value(self, parent_explore_count, uct_c):\n","        if self.outcome is not None:\n","            return self.outcome[self.player]\n","\n","        if self.explore_count == 0:\n","            return float(\"inf\")\n","\n","        return self.total_reward / self.explore_count + \\\n","            uct_c * math.sqrt(math.log(parent_explore_count) / self.explore_count)\n","\n","    # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç PUCT –¥–æ—á–µ—Ä–Ω–µ–≥–æ —É–∑–ª–∞\n","    def puct_value(self, parent_explore_count, uct_c):\n","        if self.outcome is not None:\n","            return self.outcome[self.player]\n","\n","        return ((self.explore_count and self.total_reward / self.explore_count) +\n","                uct_c * self.prior * math.sqrt(parent_explore_count) /\n","                (self.explore_count + 1))\n","\n","    # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ª—É—á—à–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ –≤ –≤–µ—Ä—à–∏–Ω–µ, –ª–∏–±–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω–æ–µ –∏–ª–∏ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Å–µ—â–∞–µ–º–æ–µ\n","    def sort_key(self):\n","        \"\"\" –¢–∞–∫–æ–π –ø–æ—Ä—è–¥–æ–∫ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –≤—ã–±–æ—Ä—É:\n","        - –ù–∞–∏–≤—ã—Å—à–µ–≥–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ > 0 over anything else, including a promising but unproven action.\n","        - –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω–∞—è –Ω–∏—á—å—è, —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–ª–∞—Å—å —á–∞—â–µ –¥—Ä—É–≥–∏—Ö –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∏–ª–∏ –ø—Ä–æ–∏–≥—Ä—ã—à–Ω—ã—Ö.\n","        - –ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ with most exploration over loss of any difficulty\n","        - –°–∞–º—ã–π —Å–ª–æ–∂–Ω—ã–π –ø—Ä–æ–∏–≥—Ä—ã—à, –µ—Å–ª–∏ –≤—Å–µ –ø—Ä–æ–∏–≥—Ä—ã—à–∏\n","        - Highest expected reward if explore counts are equal (–º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–Ω–æ).\n","        - Longest win, if multiple are proven (unlikely due to early stopping).\n","        \"\"\"\n","        return (0 if self.outcome is None else self.outcome[self.player],\n","                self.explore_count,\n","                self.total_reward)\n","\n","    # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ª—É—á—à–∏–π –¥–æ—á–µ—Ä–Ω–∏–π —É–∑–µ–ª –≤ –ø–æ—Ä—è–¥–∫–µ –∫–ª—é—á–∞ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏\n","    def best_child(self):\n","        return max(self.children, key=SearchNode.sort_key)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"hM-yrjaoTXmb","executionInfo":{"status":"ok","timestamp":1720256880644,"user_tz":-180,"elapsed":4,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"}}},"outputs":[],"source":["# –ë–æ—Ç, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π Monte-Carlo Tree Search –∞–ª–≥–æ—Ä–∏—Ç–º\n","class MCTSBot(object):\n","    def __init__(self,\n","                 uct_c,\n","                 max_simulations,\n","                 evaluator,\n","                 solve=True,\n","                 random_state=None,\n","                 child_selection_fn=SearchNode.uct_value,\n","                 dirichlet_noise=None,\n","                 verbose=False):\n","\n","        self.max_utility = 1.0  #game.max_utility()\n","        self.uct_c = uct_c\n","        self.max_simulations = max_simulations\n","        self.evaluator = evaluator\n","        self.verbose = verbose\n","        self.solve = solve\n","\n","        self._dirichlet_noise = dirichlet_noise\n","        self._random_state = random_state or np.random.RandomState()\n","        self._child_selection_fn = child_selection_fn\n","\n","    # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–∏—Ç–∏–∫—É –±–æ—Ç–∞ –∏ –¥–µ–π—Å—Ç–≤–∏–µ –≤ –¥–∞–Ω–Ω–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏\n","    def step_with_policy(self, state):\n","        t1 = time.time()\n","        root = self.mcts_search(state)\n","        best = root.best_child()\n","\n","        if self.verbose:\n","            seconds = time.time() - t1\n","            print(\"Finished {} sims in {:.3f} secs, {:.1f} sims/s\".format(\n","                root.explore_count, seconds, root.explore_count / seconds))\n","\n","        mcts_action = best.action\n","        policy = [(action, (1.0 if action == mcts_action else 0.0))\n","                  for action in state.legal_actions(state.current_player())]\n","\n","        return policy, mcts_action\n","\n","    def step(self, state):\n","        root = self.mcts_search(state)\n","        return root.best_child().action\n","\n","    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ UCT –ø–æ–ª–∏—Ç–∏–∫–∏ –¥–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ª–∏—Å—Ç–æ–≤–æ–π –≤–µ—Ä—à–∏–Ω—ã\n","    def _apply_tree_policy(self, root, state):\n","        \"\"\" –õ–∏—Å—Ç–æ–≤–∞—è –≤–µ—Ä—à–∏–Ω–∞ - —Ç–µ—Ä–º–∏–Ω–∞–ª—å–Ω–∞—è –∏–ª–∏ –µ—â–µ –Ω–µ–æ—Ü–µ–Ω–µ–Ω–Ω–∞—è –≤–µ—Ä—à–∏–Ω–∞.\n","\n","        Args:\n","            root: The root node in the search tree.\n","            state: The state of the game at the root node.\n","\n","        Returns:\n","            visit_path: –õ–∏—Å—Ç –≤–µ—Ä—à–∏–Ω –æ—Ç –∫–æ—Ä–Ω—è –∫ –∫–æ–Ω–µ—á–Ω–æ–º—É —É–∑–ª—É\n","            working_state: The state of the game at the leaf node.\n","        \"\"\"\n","\n","        visit_path = [root]\n","        working_state = state.clone()\n","        current_node = root\n","        while (not working_state.is_terminal() and current_node.explore_count > 0):\n","            # If it reaches a node that has been evaluated before but hasn't been expanded,\n","            # then expand it's children and continue.\n","            if not current_node.children:\n","                # For a new node, initialize its state, then choose a child as normal.\n","                legal_actions = self.evaluator.prior(working_state)\n","                if current_node is root and self._dirichlet_noise:\n","                    epsilon, alpha = self._dirichlet_noise\n","                    noise = self._random_state.dirichlet([alpha] * len(legal_actions))\n","                    legal_actions = [(a, (1 - epsilon) * p + epsilon * n)\n","                                    for (a, p), n in zip(legal_actions, noise)]\n","                # Reduce bias from move generation order.\n","                self._random_state.shuffle(legal_actions)\n","                player = working_state.current_player()\n","                current_node.children = [SearchNode(action, player, prior) for action, prior in legal_actions]\n","\n","            chosen_child = max(\n","                current_node.children,\n","                key=lambda c: self._child_selection_fn(c, current_node.explore_count, self.uct_c)\n","            )\n","\n","            working_state.apply_action(chosen_child.action)\n","            current_node = chosen_child\n","            visit_path.append(current_node)\n","\n","        return visit_path, working_state\n","\n","    # –í–∞–Ω–∏–ª—å–Ω—ã–π Monte-Carlo Tree Search –∞–ª–≥–æ—Ä–∏—Ç–º\n","    def mcts_search(self, state):\n","        root = SearchNode(None, state.current_player(), 1)\n","        for _ in range(self.max_simulations):\n","            visit_path, working_state = self._apply_tree_policy(root, state)\n","            if working_state.is_terminal():\n","                returns = working_state.returns()\n","                visit_path[-1].outcome = returns\n","                solved = self.solve\n","            else:\n","                returns = self.evaluator.evaluate(working_state)\n","                solved = False\n","\n","            while visit_path:\n","                node = visit_path.pop()\n","                node.total_reward += returns[node.player]\n","                node.explore_count += 1\n","\n","                if solved and node.children:\n","                    # If any have max utility (won?), or all children are solved,\n","                    # choose the one best for the player choosing.\n","                    best = None\n","                    all_solved = True\n","                    for child in node.children:\n","                        if child.outcome is None:\n","                            all_solved = False\n","                        elif best is None or child.outcome[node.player] > best.outcome[node.player]:\n","                            best = child\n","                    if (best is not None and (all_solved or best.outcome[node.player] == self.max_utility)):\n","                        node.outcome = best.outcome\n","                    else:\n","                        solved = False\n","            if root.outcome is not None:\n","                break\n","\n","        return root"]},{"cell_type":"markdown","source":["## –¢–µ—Å—Ç"],"metadata":{"id":"LeHJdQdPgXJn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ulS5eZCbuwxX"},"outputs":[],"source":["eval = RandomRolloutEvaluator(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N4hxT9oHTqUo"},"outputs":[],"source":["state = State_TicTacToe(3, 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"laU_HI6cunhB"},"outputs":[],"source":["bot = MCTSBot(uct_c=1,\n","              max_simulations=200,\n","              evaluator=eval,\n","              solve=True,\n","              child_selection_fn=SearchNode.uct_value)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"exG9um8aoyMe"},"outputs":[],"source":["root = bot.mcts_search(state)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6088,"status":"ok","timestamp":1719177128455,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"},"user_tz":-180},"id":"f-YXLCrZtKaM","outputId":"984881ad-8cbd-4ffb-bc59-e00829b81703"},"outputs":[{"output_type":"stream","name":"stdout","text":["player -1's turn:\n",". . .\n",". . .\n",". . .\n","(1, 0) -0.4000000000000001 None 7\n","(0, 2) 14.000000000000002 None 32\n","(2, 2) 9.800000000000002 None 25\n","(0, 0) 10.799999999999997 None 27\n","(1, 2) 3.0 None 15\n","(0, 1) 4.6 None 16\n","(2, 0) 5.6 None 18\n","(1, 1) 22.6 None 45\n","(2, 1) 3.2000000000000006 None 14\n","player 1's turn:\n"," .  .  .\n"," .  X  .\n"," .  .  .\n","(0, 2) -17.600000000000005 None 44\n","(1, 0) -9.0 None 12\n","(2, 0) -15.8 None 35\n","(0, 0) -11.6 None 19\n","(0, 1) -8.399999999999999 None 11\n","(2, 1) -12.0 None 21\n","(1, 2) -11.2 None 18\n","(2, 2) -16.6 None 39\n","player -1's turn:\n"," .  .  O\n"," .  X  .\n"," .  .  .\n","(0, 0) 11.600000000000001 None 31\n","(2, 0) 5.6 None 21\n","(1, 2) 13.4 None 33\n","(2, 1) 1.6 None 12\n","(2, 2) 11.599999999999998 None 30\n","(0, 1) 27.0 None 53\n","(1, 0) 5.200000000000001 None 19\n","player 1's turn:\n"," .  X  O\n"," .  X  .\n"," .  .  .\n","(2, 0) -9.8 None 13\n","(1, 0) -12.6 None 20\n","(2, 2) -17.599999999999998 None 37\n","(1, 2) -21.600000000000005 None 50\n","(0, 0) -10.4 None 15\n","(2, 1) -25.4 None 64\n","player -1's turn:\n"," .  X  O\n"," .  X  .\n"," .  O  .\n","(1, 0) 8.399999999999999 None 38\n","(0, 0) -1.2000000000000002 None 12\n","(1, 2) 11.799999999999999 None 45\n","(2, 2) 19.6 None 63\n","(2, 0) 9.8 None 41\n","player 1's turn:\n"," .  X  O\n"," .  X  .\n"," .  O  X\n","(1, 2) -7.4 [ 1 -1] 12\n","(0, 0) -5.2 [0 0] 169\n","(1, 0) -6.6 [ 1 -1] 12\n","(2, 0) -6.0 None 6\n","player -1's turn:\n"," O  X  O\n"," .  X  .\n"," .  O  X\n","(1, 0) 1.4 None 4\n","(1, 2) 2.8 [ 1 -1] 5\n","(2, 0) 0.0 None 2\n","player 1's turn:\n"," O  X  O\n"," .  X  X\n"," .  O  X\n","(2, 0) -2.0 [ 1 -1] 2\n","(1, 0) 0.0 [0 0] 2\n","player -1's turn:\n"," O  X  O\n"," O  X  X\n"," .  O  X\n","(2, 0) 0.0 [0 0] 1\n","player 0's turn:\n"," O  X  O\n"," O  X  X\n"," X  O  X\n"]}],"source":["state.restart()\n","state.visualize_state()\n","\n","while not state.is_terminal():\n","    #action = bot.step(state)\n","\n","    root = bot.mcts_search(state)\n","    for child in root.children:\n","        print(child.action, child.total_reward, child.outcome, child.explore_count)\n","    action = root.best_child().action\n","\n","    state.apply_action(action)\n","    state.visualize_state()"]},{"cell_type":"markdown","source":["#–ú–æ–¥–µ–ª—å"],"metadata":{"id":"_SRkR1aDtBGd"}},{"cell_type":"code","source":["\"\"\"An AlphaZero style model with a policy and value head\"\"\"\n","\n","import collections\n","import functools\n","import os\n","from typing import Sequence\n","\n","import numpy as np\n","import tensorflow.compat.v1 as tf"],"metadata":{"id":"MknvROBItJPw","executionInfo":{"status":"ok","timestamp":1720256891274,"user_tz":-180,"elapsed":10136,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def cascade(x, fns):\n","    for fn in fns:\n","        x = fn(x)\n","    return x\n","\n","tfkl = tf.keras.layers\n","conv_2d = functools.partial(tfkl.Conv2D, padding=\"same\")\n","\n","\n","def batch_norm(training, updates, name):\n","    \"\"\"A batch norm layer.\n","\n","    Args:\n","      training: A placeholder of whether this is done in training or not.\n","      updates: A list to be extended with this layer's updates.\n","      name: Name of the layer.\n","\n","    Returns:\n","      A function to apply to the previous layer.\n","    \"\"\"\n","    bn = tfkl.BatchNormalization(name=name)\n","    def batch_norm_layer(x):\n","        # This emits a warning that training is a placeholder instead of a concrete\n","        # bool, but seems to work anyway.\n","        applied = bn(x, training)\n","        updates.extend(bn.updates)\n","        return applied\n","    return batch_norm_layer\n","\n","\n","def residual_layer(inputs, num_filters, kernel_size, training, updates, name):\n","  return cascade(inputs, [\n","      conv_2d(num_filters, kernel_size, name=f\"{name}_res_conv1\"),\n","      batch_norm(training, updates, f\"{name}_res_batch_norm1\"),\n","      tfkl.Activation(\"relu\"),\n","      conv_2d(num_filters, kernel_size, name=f\"{name}_res_conv2\"),\n","      batch_norm(training, updates, f\"{name}_res_batch_norm2\"),\n","      lambda x: tfkl.add([x, inputs]),\n","      tfkl.Activation(\"relu\"),\n","  ])\n","\n","# Inputs for training the Model\n","class TrainInput(collections.namedtuple(\"TrainInput\", \"observation legals_mask policy value\")):\n","    @staticmethod\n","    def stack(train_inputs):\n","        observation, legals_mask, policy, value = zip(*train_inputs)\n","        return TrainInput(\n","            np.array(observation, dtype=np.float32),\n","            np.array(legals_mask, dtype=bool),\n","            np.array(policy),\n","            np.expand_dims(value, 1)\n","        )\n","\n","\n","# An AlphaZero style model with a policy and value head\n","class Model(object):\n","    # Init a model. Use build_model, from_checkpoint or from_graph instead\n","    def __init__(self, session, saver, path):\n","        self._session = session\n","        self._saver = saver\n","        self._path = path\n","\n","        def get_var(name):\n","            return self._session.graph.get_tensor_by_name(name + \":0\")\n","\n","        self._input = get_var(\"input\")\n","        self._legals_mask = get_var(\"legals_mask\")\n","        self._training = get_var(\"training\")\n","        self._value_out = get_var(\"value_out\")\n","        self._policy_softmax = get_var(\"policy_softmax\")\n","        self._policy_loss = get_var(\"policy_loss\")\n","        self._value_loss = get_var(\"value_loss\")\n","        self._l2_reg_loss = get_var(\"l2_reg_loss\")\n","        self._policy_targets = get_var(\"policy_targets\")\n","        self._value_targets = get_var(\"value_targets\")\n","        self._train = self._session.graph.get_operation_by_name(\"train\")\n","\n","    # Build a model with the specified params\n","    @classmethod\n","    def build_model(cls, model_type, input_shape, output_size, nn_width, nn_depth,\n","                    weight_decay, learning_rate, path):\n","        g = tf.Graph()  # Allow multiple independent models and graphs.\n","        with g.as_default():\n","            cls._define_graph(model_type, input_shape, output_size, nn_width,\n","                              nn_depth, weight_decay, learning_rate)\n","            init = tf.variables_initializer(tf.global_variables(), name=\"init_all_vars_op\")\n","            with tf.device(\"/cpu:0\"):  # Saver only works on CPU.\n","                saver = tf.train.Saver(max_to_keep=10000, sharded=False, name=\"saver\")\n","        session = tf.Session(graph=g)\n","        session.__enter__()\n","        session.run(init)\n","        return cls(session, saver, path)\n","\n","    # Load a model from a checkpoint\n","    @classmethod\n","    def from_checkpoint(cls, checkpoint, path=None):\n","        model = cls.from_graph(checkpoint, path)\n","        model.load_checkpoint(checkpoint)\n","        return model\n","\n","    # Load only the model from a graph or checkpoint\n","    @classmethod\n","    def from_graph(cls, metagraph, path=None):\n","        if not os.path.exists(metagraph):\n","            metagraph += \".meta\"\n","        if not path:\n","            path = os.path.dirname(metagraph)\n","        g = tf.Graph()  # Allow multiple independent models and graphs.\n","        with g.as_default():\n","            saver = tf.train.import_meta_graph(metagraph)\n","        session = tf.Session(graph=g)\n","        session.__enter__()\n","        session.run(\"init_all_vars_op\")\n","        return cls(session, saver, path)\n","\n","    def __del__(self):\n","        if hasattr(self, \"_session\") and self._session:\n","            self._session.close()\n","\n","    # Define the model graph\n","    @staticmethod\n","    def _define_graph(model_type, input_shape, output_size, nn_width, nn_depth, weight_decay, learning_rate):\n","        # Inference inputs\n","        input_size = int(np.prod(input_shape))\n","        observations = tf.placeholder(tf.float32, [None, input_size], name=\"input\")\n","        legals_mask = tf.placeholder(tf.bool, [None, output_size], name=\"legals_mask\")\n","        training = tf.placeholder(tf.bool, name=\"training\")\n","\n","        bn_updates = []\n","\n","        # Main torso of the network\n","        if model_type == \"resnet\":\n","            torso = cascade(observations, [\n","                tfkl.Reshape(input_shape),\n","                conv_2d(nn_width, 3, name=\"torso_in_conv\"),\n","                batch_norm(training, bn_updates, \"torso_in_batch_norm\"),\n","                tfkl.Activation(\"relu\"),\n","            ])\n","            for i in range(nn_depth):\n","                torso = residual_layer(torso, nn_width, 3, training, bn_updates, f\"torso_{i}\")\n","\n","        policy_head = cascade(torso, [\n","            conv_2d(filters=2, kernel_size=1, name=\"policy_conv\"),\n","            batch_norm(training, bn_updates, \"policy_batch_norm\"),\n","            tfkl.Activation(\"relu\"),\n","            tfkl.Flatten(),\n","        ])\n","        policy_logits = tfkl.Dense(output_size, name=\"policy\")(policy_head)\n","        policy_logits = tf.where(legals_mask, policy_logits, -1e32 * tf.ones_like(policy_logits))\n","        unused_policy_softmax = tf.identity(tfkl.Softmax()(policy_logits), name=\"policy_softmax\")\n","        policy_targets = tf.placeholder(shape=[None, output_size], dtype=tf.float32, name=\"policy_targets\")\n","        policy_loss = tf.reduce_mean(\n","            tf.nn.softmax_cross_entropy_with_logits_v2(logits=policy_logits, labels=policy_targets),\n","            name=\"policy_loss\"\n","        )\n","\n","        value_head = cascade(torso, [\n","            conv_2d(filters=1, kernel_size=1, name=\"value_conv\"),\n","            batch_norm(training, bn_updates, \"value_batch_norm\"),\n","            tfkl.Activation(\"relu\"),\n","            tfkl.Flatten(),\n","        ])\n","        value_out = cascade(value_head, [\n","            tfkl.Dense(nn_width, name=\"value_dense\"),\n","            tfkl.Activation(\"relu\"),\n","            tfkl.Dense(1, name=\"value\"),\n","            tfkl.Activation(\"tanh\"),\n","        ])\n","        # Need the identity to name the single value output from the dense layer.\n","        value_out = tf.identity(value_out, name=\"value_out\")\n","        value_targets = tf.placeholder(shape=[None, 1], dtype=tf.float32, name=\"value_targets\")\n","        value_loss = tf.identity(tf.losses.mean_squared_error(value_out, value_targets), name=\"value_loss\")\n","\n","        l2_reg_loss = tf.add_n([\n","            weight_decay * tf.nn.l2_loss(var)\n","            for var in tf.trainable_variables()\n","            if \"/bias:\" not in var.name\n","        ], name=\"l2_reg_loss\")\n","\n","        total_loss = policy_loss + value_loss + l2_reg_loss\n","        optimizer = tf.train.AdamOptimizer(learning_rate)\n","        with tf.control_dependencies(bn_updates):\n","            unused_train = optimizer.minimize(total_loss, name=\"train\")\n","\n","    @property\n","    def num_trainable_variables(self):\n","        return sum(np.prod(v.shape) for v in tf.trainable_variables())\n","\n","    def inference(self, observation, legals_mask):\n","        return self._session.run(\n","            [self._value_out, self._policy_softmax],\n","            feed_dict={self._input: np.array(observation, dtype=np.float32),\n","                       self._legals_mask: np.array(legals_mask, dtype=bool),\n","                       self._training: False})\n","\n","    # Runs a training step\n","    def update(self, train_inputs: Sequence[TrainInput]):\n","        batch = TrainInput.stack(train_inputs)\n","\n","        # Run a training step and get the losses.\n","        _, policy_loss, value_loss, l2_reg_loss = self._session.run(\n","            [self._train, self._policy_loss, self._value_loss, self._l2_reg_loss],\n","            feed_dict={self._input: batch.observation,\n","                       self._legals_mask: batch.legals_mask,\n","                       self._policy_targets: batch.policy,\n","                       self._value_targets: batch.value,\n","                       self._training: True})\n","\n","        return policy_loss, value_loss, l2_reg_loss\n","\n","    def save_checkpoint(self, step):\n","        return self._saver.save(\n","            self._session,\n","            os.path.join(self._path, \"checkpoint\"),\n","            global_step=step)\n","\n","    def load_checkpoint(self, path):\n","        return self._saver.restore(self._session, path)"],"metadata":{"id":"YVlOT707tDQ2","executionInfo":{"status":"ok","timestamp":1720256891275,"user_tz":-180,"elapsed":5,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# Alpha Zero"],"metadata":{"id":"H9NSkuK5mqTj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OteIJfC2mdnm"},"outputs":[],"source":["!pip install open_spiel"]},{"cell_type":"code","source":["import numpy as np\n","import collections\n","import traceback\n","import functools\n","import itertools\n","import datetime\n","import random\n","import time\n","import json\n","import os\n","\n","#from open_spiel.python.algorithms.alpha_zero import model as model_lib\n","import pyspiel\n","\n","from open_spiel.python.utils import lru_cache\n","from open_spiel.python.utils import file_logger\n","from open_spiel.python.utils import spawn\n","from open_spiel.python.utils import stats"],"metadata":{"id":"AFRoQuS1mloz","executionInfo":{"status":"ok","timestamp":1720256891275,"user_tz":-180,"elapsed":4,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# An AlphaZero MCTS Evaluator\n","class AlphaZeroEvaluator(object):\n","    def __init__(self, model, cache_size=2**16):\n","        self._model = model\n","        self._cache = lru_cache.LRUCache(cache_size)\n","\n","    def cache_info(self):\n","        return self._cache.info()\n","\n","    def clear_cache(self):\n","        self._cache.clear()\n","\n","    def _inference(self, state):\n","        # Make a singleton batch\n","        obs = np.expand_dims(state.observation_tensor(), 0)\n","        mask = np.expand_dims(state.legal_actions_mask(), 0)\n","\n","        # ndarray isn't hashable\n","        cache_key = obs.tobytes() + mask.tobytes()\n","        value, policy = self._cache.make(cache_key, lambda: self._model.inference(obs, mask))\n","        return value[0, 0], policy[0]  # Unpack batch\n","\n","    # Returns a value for the given state\n","    def evaluate(self, state):\n","        value, _ = self._inference(state)\n","        return np.array([value, -value])\n","\n","    # Returns the probabilities for all actions.\n","    def prior(self, state):\n","        _, policy = self._inference(state)\n","        return [(action, policy[action]) for action in state.legal_actions()]"],"metadata":{"id":"MJC4DnnFvqsi","executionInfo":{"status":"ok","timestamp":1720256891933,"user_tz":-180,"elapsed":661,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Time to wait for processes to join.\n","JOIN_WAIT_DELAY = 0.001\n","\n","\n","# A particular point along a trajectory\n","class TrajectoryState(object):\n","    def __init__(self, observation, current_player, legals_mask, action, policy, value):\n","        self.observation = observation\n","        self.current_player = current_player\n","        self.legals_mask = legals_mask\n","        self.action = action\n","        self.policy = policy\n","        self.value = value\n","\n","\n","# A sequence of (observations, actions and policies), and the outcomes\n","class Trajectory(object):\n","    def __init__(self):\n","        self.states = []\n","        self.returns = None\n","\n","\n","# A fixed size buffer that keeps the newest values\n","class Buffer(object):\n","    def __init__(self, max_size):\n","        self.max_size = max_size\n","        self.data = []\n","        self.total_seen = 0  # The number of items that have passed through.\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def append(self, val):\n","        return self.extend([val])\n","\n","    def extend(self, batch):\n","        batch = list(batch)\n","        self.total_seen += len(batch)\n","        self.data.extend(batch)\n","        self.data[:-self.max_size] = []\n","\n","    def sample(self, count):\n","        return random.sample(self.data, count)\n","\n","\n","# A config for the model/experiment\n","class Config(collections.namedtuple(\n","    \"Config\", [\n","        \"game\",\n","        \"path\",\n","        \"learning_rate\",\n","        \"weight_decay\",\n","        \"train_batch_size\",\n","        \"replay_buffer_size\",\n","        \"replay_buffer_reuse\",\n","        \"max_steps\",\n","        \"checkpoint_freq\",\n","        \"actors\",\n","        \"evaluators\",\n","        \"evaluation_window\",\n","        \"eval_levels\",\n","\n","        \"uct_c\",\n","        \"max_simulations\",\n","        \"policy_alpha\",\n","        \"policy_epsilon\",\n","        \"temperature\",\n","        \"temperature_drop\",\n","\n","        \"nn_model\",\n","        \"nn_width\",\n","        \"nn_depth\",\n","        \"observation_shape\",\n","        \"output_size\",\n","\n","        \"quiet\",\n","    ])):\n","  pass\n","\n","\n","def _init_model_from_config(config):\n","    return Model.build_model(\n","        config.nn_model,\n","        config.observation_shape,\n","        config.output_size,\n","        config.nn_width,\n","        config.nn_depth,\n","        config.weight_decay,\n","        config.learning_rate,\n","        config.path)\n","\n","\n","# A decorator to fn/processes that gives a logger and logs exceptions\n","def watcher(fn):\n","    # Wrap the decorated function\n","    @functools.wraps(fn)\n","    def _watcher(*, config, num=None, **kwargs):\n","        name = fn.__name__\n","        if num is not None:\n","            name += \"-\" + str(num)\n","        with file_logger.FileLogger(config.path, name, config.quiet) as logger:\n","            print(f'{name} started')\n","            logger.print(f\"{name} started\")\n","            try:\n","                return fn(config=config, logger=logger, **kwargs)\n","            except Exception as e:\n","                logger.print(f\"\\n{' Exception caught '.center(60, '=')}{traceback.format_exc()}{'=' * 60}\")\n","                print(f\"Exception caught in {name}: {e}\")\n","                raise\n","            finally:\n","                logger.print(f\"{name} exiting\")\n","                print(f\"{name} exiting\", end='\\n\\n')\n","    return _watcher\n","\n","\n","# Initializes a bot\n","def _init_bot(config, game, evaluator_, evaluation):\n","    noise = None if evaluation else (config.policy_epsilon, config.policy_alpha)\n","    return MCTSBot(\n","        config.uct_c,\n","        config.max_simulations,\n","        evaluator_,\n","        solve=False,\n","        dirichlet_noise=noise,\n","        child_selection_fn=SearchNode.puct_value,\n","        verbose=False)"],"metadata":{"id":"fmA5rdJJqGV9","executionInfo":{"status":"ok","timestamp":1720256891934,"user_tz":-180,"elapsed":6,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Play one game, return the trajectory\n","def _play_game(logger, game_num, game, bots, temperature, temperature_drop):\n","    trajectory = Trajectory()\n","    actions = []\n","    state = game.new_initial_state()\n","    random_state = np.random.RandomState()\n","    #logger.opt_print(\" Starting game {} \".format(game_num).center(60, \"-\"))\n","    #logger.opt_print(\"Initial state:\\n{}\".format(state))\n","    while not state.is_terminal():\n","        root = bots[state.current_player()].mcts_search(state)\n","        policy = np.zeros(game.num_distinct_actions())\n","        for c in root.children:\n","            policy[c.action] = c.explore_count\n","        policy = policy**(1 / temperature)\n","        policy /= policy.sum()\n","        if len(actions) >= temperature_drop:\n","            action = root.best_child().action\n","        else:\n","            action = np.random.choice(len(policy), p=policy)\n","        trajectory.states.append(\n","            TrajectoryState(state.observation_tensor(), state.current_player(),\n","                            state.legal_actions_mask(), action, policy,\n","                            root.total_reward / root.explore_count))\n","        action_str = state.action_to_string(state.current_player(), action)\n","        actions.append(action_str)\n","        #logger.opt_print(\"Player {} sampled action: {}\".format(state.current_player(), action_str))\n","        state.apply_action(action)\n","    #logger.opt_print(\"Next state:\\n{}\".format(state))\n","\n","    trajectory.returns = state.returns()\n","    #logger.print(\"Game {}: Returns: {}; Actions: {}\".format(game_num, \" \".join(map(str, trajectory.returns)), \" \".join(actions)))\n","    return trajectory\n","\n","\n","# Read the queue for a checkpoint to load, or an exit signal\n","def update_checkpoint(logger, queue, model, az_evaluator):\n","    path = None\n","    while True:  # Get the last message, ignore intermediate ones.\n","        try:\n","            path = queue.get_nowait()\n","        except spawn.Empty:\n","            break\n","    if path:\n","        logger.print(\"Inference cache:\", az_evaluator.cache_info())\n","        logger.print(\"Loading checkpoint\", path)\n","        model.load_checkpoint(path)\n","        az_evaluator.clear_cache()\n","    elif path is not None:  # Empty string means stop this process.\n","        return False\n","    return True\n","\n","\n","# An actor process runner that generates games and returns trajectories\n","@watcher\n","def actor(*, config, game, logger, queue):\n","    logger.print(\"Initializing model\")\n","    model = _init_model_from_config(config)\n","\n","    logger.print(\"Initializing bots\")\n","    az_evaluator = AlphaZeroEvaluator(model)\n","    bots = [_init_bot(config, game, az_evaluator, False),\n","            _init_bot(config, game, az_evaluator, False)]\n","\n","    for game_num in itertools.count():\n","        if not update_checkpoint(logger, queue, model, az_evaluator):\n","            return\n","        queue.put(_play_game(logger, game_num, game, bots, config.temperature, config.temperature_drop))\n","\n","\n","# A process that plays the latest checkpoint vs standard MCTS\n","@watcher\n","def evaluator(*, game, config, logger, queue):\n","    results = Buffer(config.evaluation_window)\n","\n","    logger.print(\"Initializing model\")\n","    model = _init_model_from_config(config)\n","\n","    logger.print(\"Initializing bots\")\n","    az_evaluator = AlphaZeroEvaluator(model)\n","    random_evaluator = RandomRolloutEvaluator()\n","\n","    for game_num in itertools.count():\n","        if not update_checkpoint(logger, queue, model, az_evaluator):\n","            return\n","\n","        az_player = game_num % 2\n","        difficulty = (game_num // 2) % config.eval_levels\n","        max_simulations = int(config.max_simulations * (10 ** (difficulty / 2)))\n","        bots = [\n","            _init_bot(config, game, az_evaluator, True),\n","            MCTSBot(\n","                config.uct_c,\n","                max_simulations,\n","                random_evaluator,\n","                solve=True,\n","                verbose=False)\n","        ]\n","        if az_player == 1:\n","            bots = list(reversed(bots))\n","\n","        trajectory = _play_game(logger, game_num, game, bots, temperature=1, temperature_drop=0)\n","        results.append(trajectory.returns[az_player])\n","\n","        logger.print(f\"AZ: {trajectory.returns[az_player]}, MCTS: {trajectory.returns[1 - az_player]}, AZ avg/{len(results)}: {np.mean(results.data):.3f}\")"],"metadata":{"id":"V0s1yNcSWoIl","executionInfo":{"status":"ok","timestamp":1720256891934,"user_tz":-180,"elapsed":5,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"id":"01msbPPk4KYu","executionInfo":{"status":"ok","timestamp":1720256891935,"user_tz":-180,"elapsed":5,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"}}},"outputs":[],"source":["# A learner that consumes the replay buffer and trains the network\n","@watcher\n","def learner(*, game, config, actors, evaluators, broadcast_fn, logger):\n","    logger.also_to_stdout = True\n","    replay_buffer = Buffer(config.replay_buffer_size)\n","    learn_rate = config.replay_buffer_size // config.replay_buffer_reuse\n","\n","    logger.print(\"Initializing model\")\n","    model = _init_model_from_config(config)\n","    logger.print(\"Model type: %s(%s, %s)\" % (config.nn_model, config.nn_width, config.nn_depth))\n","    logger.print(\"Model size:\", model.num_trainable_variables, \"variables\")\n","\n","    save_path = model.save_checkpoint(0)\n","    logger.print(\"Initial checkpoint:\", save_path)\n","    broadcast_fn(save_path)\n","\n","    game_lengths = stats.BasicStats()\n","    game_lengths_hist = stats.HistogramNumbered(game.max_game_length() + 1)\n","    outcomes = stats.HistogramNamed([\"Player1\", \"Player2\", \"Draw\"])\n","    total_trajectories = 0\n","\n","    # Merge all the actor queues into a single generator\n","    def trajectory_generator():\n","        while True:\n","            found = 0\n","            for actor_process in actors:\n","                try:\n","                    yield actor_process.queue.get_nowait()\n","                except spawn.Empty:\n","                    pass\n","                else:\n","                    found += 1\n","            if found == 0:\n","                time.sleep(0.01)  # 10ms\n","\n","    # Collects the trajectories from actors into the replay buffer\n","    def collect_trajectories():\n","        num_trajectories = 0\n","        num_states = 0\n","        for trajectory in trajectory_generator():\n","            num_trajectories += 1\n","            num_states += len(trajectory.states)\n","            game_lengths.add(len(trajectory.states))\n","            game_lengths_hist.add(len(trajectory.states))\n","\n","            p1_outcome = trajectory.returns[0]\n","            outcomes.add(0*(p1_outcome > 0) + 1*(p1_outcome < 0) + 2*(p1_outcome == 0))\n","\n","            replay_buffer.extend(\n","                TrainInput(s.observation, s.legals_mask, s.policy, p1_outcome)\n","                for s in trajectory.states)\n","\n","            if num_states >= learn_rate:\n","                break\n","        return num_trajectories, num_states\n","\n","    # Sample from the replay buffer, update weights and save a checkpoint\n","    def learn(step):\n","        losses = []\n","        for _ in range(len(replay_buffer) // config.train_batch_size):\n","            data = replay_buffer.sample(config.train_batch_size)\n","            policy_loss, value_loss, l2_reg_loss = model.update(data)\n","            losses.append([policy_loss, value_loss, l2_reg_loss])\n","\n","        # Always save a checkpoint, either for keeping or for loading the weights to\n","        # the actors. It only allows numbers, so use -1 as \"latest\".\n","        save_path = model.save_checkpoint(\n","            step if step % config.checkpoint_freq == 0 else -1)\n","\n","        policy_loss, value_loss, l2_reg_loss = np.mean(losses, axis=0)\n","        total_loss = policy_loss + value_loss + l2_reg_loss\n","        logger.print(f\"Losses(total: {total_loss:.3f}, policy: {policy_loss:.3f}, value: {value_loss:.3f}, l2: {l2_reg_loss:.3f})\")\n","        logger.print(\"Checkpoint saved:\", save_path)\n","        return save_path, (policy_loss, value_loss, l2_reg_loss)\n","\n","    last_time = time.time() - 60\n","    for step in itertools.count(1):\n","        game_lengths.reset()\n","        game_lengths_hist.reset()\n","        outcomes.reset()\n","\n","        num_trajectories, num_states = collect_trajectories()\n","        total_trajectories += num_trajectories\n","        now = time.time()\n","        seconds = now - last_time\n","        last_time = now\n","\n","        save_path, losses = learn(step)\n","\n","        logger.print(f\"Step: {step}\")\n","        logger.print((\"Collected {:5} states from {:3} games, {:.1f} states/s. \"\n","                      \"{:.1f} states/(s*actor), game length: {:.1f}\").format(\n","                num_states, num_trajectories, num_states / seconds,\n","                num_states / (config.actors * seconds),\n","                num_states / num_trajectories))\n","        logger.print(\"game_length_hist:\", game_lengths_hist.data)\n","        logger.print(\"outcomes\", outcomes.data)\n","        logger.print(f\"Buffer size: {len(replay_buffer)}. States seen: {replay_buffer.total_seen}\")\n","        logger.print()\n","\n","        if config.max_steps > 0 and step >= config.max_steps:\n","            break\n","\n","        broadcast_fn(save_path)"]},{"cell_type":"code","source":["# Start all the worker processes for a full alphazero setup\n","def alpha_zero(config: Config):\n","    # --- –ó–∞–≥—Ä—É–∑–∫–∞ –∏–≥—Ä—ã ---\n","    game = pyspiel.load_game(config.game)\n","    config = config._replace(\n","        observation_shape=game.observation_tensor_shape(),\n","        output_size=game.num_distinct_actions())\n","    print(\"Starting game\", config.game)\n","\n","    # --- –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è logs, checkpoints, config ---\n","    if not os.path.exists(config.path):\n","        os.makedirs(config.path)\n","    print(\"Writing logs and checkpoints to:\", config.path)\n","\n","    with open(os.path.join(config.path, \"config.json\"), \"w\") as fp:\n","        fp.write(json.dumps(config._asdict(), indent=2, sort_keys=True) + \"\\n\")\n","\n","    print(\"Model type: %s(%s, %s)\" % (config.nn_model, config.nn_width, config.nn_depth))\n","\n","    actors = [spawn.Process(actor, kwargs={\"game\": game, \"config\": config, \"num\": i})\n","              for i in range(config.actors)]\n","    evaluators = [spawn.Process(evaluator, kwargs={\"game\": game, \"config\": config, \"num\": i})\n","                  for i in range(config.evaluators)]\n","\n","    def broadcast(msg):\n","        for proc in actors + evaluators:\n","            proc.queue.put(msg)\n","\n","    try:\n","        learner(game=game, config=config, actors=actors, evaluators=evaluators, broadcast_fn=broadcast)\n","    except (KeyboardInterrupt, EOFError):\n","        print(\"Caught a KeyboardInterrupt, stopping early.\")\n","    finally:\n","        broadcast(\"\")\n","        # for actor processes to join we have to make sure that their q_in is empty, including backed up items\n","        for proc in actors:\n","            while proc.exitcode is None:\n","                while not proc.queue.empty():\n","                    proc.queue.get_nowait()\n","                proc.join(JOIN_WAIT_DELAY)\n","        for proc in evaluators:\n","            proc.join()"],"metadata":{"id":"WrQx11yTqTX5","executionInfo":{"status":"ok","timestamp":1720256891935,"user_tz":-180,"elapsed":5,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["config = Config(\n","    game=\"tic_tac_toe\",\n","    path=\"az-{}\".format(datetime.datetime.now().strftime(\"%d.%m.%y-%H:%M\")),\n","    learning_rate=0.01,\n","    weight_decay=1e-4,\n","    train_batch_size=128,\n","    replay_buffer_size=2**14,\n","    replay_buffer_reuse=4,\n","    max_steps=25,\n","    checkpoint_freq=25,\n","\n","    actors=4,\n","    evaluators=4,\n","    uct_c=1,\n","    max_simulations=20,\n","    policy_alpha=0.25,\n","    policy_epsilon=1,\n","    temperature=1,\n","    temperature_drop=4,\n","    evaluation_window=50,\n","    eval_levels=7,\n","\n","    nn_model=\"resnet\",\n","    nn_width=128,\n","    nn_depth=2,\n","    observation_shape=None,\n","    output_size=None,\n","\n","    quiet=True,\n",")"],"metadata":{"id":"iv8pcdWUkYdT","executionInfo":{"status":"ok","timestamp":1720256893133,"user_tz":-180,"elapsed":2,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3225373,"status":"ok","timestamp":1720045874281,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"},"user_tz":-180},"outputId":"d4cf3863-6af9-4137-8576-2a9d9a30bac1","id":"dgut7pxGdavl"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting game tic_tac_toe\n","Writing logs and checkpoints to: az-03.07.24-21:37\n","Model type: resnet(128, 2)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"output_type":"stream","name":"stdout","text":["actor-0 started\n","actor-1 started\n","actor-2 started\n","actor-3 started\n","evaluator-0 started\n","evaluator-1 started\n","evaluator-2 startedlearner started\n","[2024-07-03 21:37:28.914] Initializing model\n","\n","evaluator-3 started\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"]},{"output_type":"stream","name":"stdout","text":["[2024-07-03 21:38:00.227] Model type: resnet(128, 2)\n","[2024-07-03 21:38:00.240] Model size: 597173 variables\n","[2024-07-03 21:38:05.198] Initial checkpoint: az-03.07.24-21:37/checkpoint-0\n","[2024-07-03 21:39:34.747] Losses(total: 2.252, policy: 1.614, value: 0.519, l2: 0.119)\n","[2024-07-03 21:39:34.762] Checkpoint saved: az-03.07.24-21:37/checkpoint--1\n","[2024-07-03 21:39:34.773] Step: 1\n","[2024-07-03 21:39:34.778] Collected  4103 states from 557 games, 39.2 states/s. 9.8 states/(s*actor), game length: 7.4\n","[2024-07-03 21:39:34.790] game_length_hist: [0, 0, 0, 0, 0, 118, 43, 127, 55, 214]\n","[2024-07-03 21:39:34.815] outcomes {'counts': [273, 98, 186], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-03 21:39:34.833] Buffer size: 4103. States seen: 4103\n","[2024-07-03 21:39:34.847] Loss. policy: 1.6136566027998924. value: 0.519357968121767. l2reg: 0.11893886514008045. sum: 2.25195343606174\n","[2024-07-03 21:39:34.858]\n","[2024-07-03 21:40:53.072] Losses(total: 2.216, policy: 1.590, value: 0.488, l2: 0.139)\n","[2024-07-03 21:40:53.086] Checkpoint saved: az-03.07.24-21:37/checkpoint--1\n","[2024-07-03 21:40:53.092] Step: 2\n","[2024-07-03 21:40:53.103] Collected  4102 states from 549 games, 87.1 states/s. 21.8 states/(s*actor), game length: 7.5\n","[2024-07-03 21:40:53.110] game_length_hist: [0, 0, 0, 0, 0, 109, 30, 128, 57, 225]\n","[2024-07-03 21:40:53.118] outcomes {'counts': [281, 87, 181], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-03 21:40:53.129] Buffer size: 8205. States seen: 8205\n","[2024-07-03 21:40:53.142] Loss. policy: 1.5896535944193602. value: 0.4876582561992109. l2reg: 0.13893660390749574. sum: 2.216248454526067\n","[2024-07-03 21:40:53.147]\n","[2024-07-03 21:42:41.236] Losses(total: 2.161, policy: 1.599, value: 0.456, l2: 0.106)\n","[2024-07-03 21:42:41.249] Checkpoint saved: az-03.07.24-21:37/checkpoint--1\n","[2024-07-03 21:42:41.256] Step: 3\n","[2024-07-03 21:42:41.274] Collected  4097 states from 561 games, 48.8 states/s. 12.2 states/(s*actor), game length: 7.3\n","[2024-07-03 21:42:41.278] game_length_hist: [0, 0, 0, 0, 0, 125, 42, 147, 32, 215]\n","[2024-07-03 21:42:41.315] outcomes {'counts': [327, 74, 160], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-03 21:42:41.316] Buffer size: 12302. States seen: 12302\n","[2024-07-03 21:42:41.326] Loss. policy: 1.5991380165020626. value: 0.45589954188714427. l2reg: 0.10633737679260473. sum: 2.1613749351818115\n","[2024-07-03 21:42:41.366]\n","[2024-07-03 21:44:55.126] Losses(total: 2.128, policy: 1.603, value: 0.442, l2: 0.083)\n","[2024-07-03 21:44:55.134] Checkpoint saved: az-03.07.24-21:37/checkpoint--1\n","[2024-07-03 21:44:55.141] Step: 4\n","[2024-07-03 21:44:55.146] Collected  4100 states from 580 games, 40.0 states/s. 10.0 states/(s*actor), game length: 7.1\n","[2024-07-03 21:44:55.158] game_length_hist: [0, 0, 0, 0, 0, 138, 46, 201, 28, 167]\n","[2024-07-03 21:44:55.165] outcomes {'counts': [371, 74, 135], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-03 21:44:55.176] Buffer size: 16384. States seen: 16402\n","[2024-07-03 21:44:55.194] Loss. policy: 1.6028858413919806. value: 0.4420100888237357. l2reg: 0.08342998952139169. sum: 2.128325919737108\n","[2024-07-03 21:44:55.201]\n","[2024-07-03 21:47:11.251] Losses(total: 2.091, policy: 1.595, value: 0.422, l2: 0.074)\n","[2024-07-03 21:47:11.261] Checkpoint saved: az-03.07.24-21:37/checkpoint--1\n","[2024-07-03 21:47:11.266] Step: 5\n","[2024-07-03 21:47:11.268] Collected  4100 states from 569 games, 30.8 states/s. 7.7 states/(s*actor), game length: 7.2\n","[2024-07-03 21:47:11.272] game_length_hist: [0, 0, 0, 0, 0, 137, 43, 153, 38, 198]\n","[2024-07-03 21:47:11.283] outcomes {'counts': [323, 81, 165], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-03 21:47:11.292] Buffer size: 16384. States seen: 20502\n","[2024-07-03 21:47:11.295] Loss. policy: 1.5948999598622322. value: 0.42165612103417516. l2reg: 0.0743936057551764. sum: 2.0909496866515838\n","[2024-07-03 21:47:11.312]\n","[2024-07-03 21:49:22.835] Losses(total: 2.049, policy: 1.580, value: 0.396, l2: 0.072)\n","[2024-07-03 21:49:22.844] Checkpoint saved: az-03.07.24-21:37/checkpoint--1\n","[2024-07-03 21:49:22.850] Step: 6\n","[2024-07-03 21:49:22.853] Collected  4097 states from 552 games, 30.0 states/s. 7.5 states/(s*actor), game length: 7.4\n","[2024-07-03 21:49:22.858] game_length_hist: [0, 0, 0, 0, 0, 111, 37, 133, 50, 221]\n","[2024-07-03 21:49:22.873] outcomes {'counts': [273, 87, 192], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-03 21:49:22.877] Buffer size: 16384. States seen: 24599\n","[2024-07-03 21:49:22.887] Loss. policy: 1.5804999992251396. value: 0.39569071866571903. l2reg: 0.07233721553348005. sum: 2.0485279334243387\n","[2024-07-03 21:49:22.899]\n","[2024-07-03 21:51:33.055] Losses(total: 2.053, policy: 1.584, value: 0.398, l2: 0.070)\n","[2024-07-03 21:51:33.063] Checkpoint saved: az-03.07.24-21:37/checkpoint--1\n","[2024-07-03 21:51:33.079] Step: 7\n","[2024-07-03 21:51:33.089] Collected  4100 states from 560 games, 31.2 states/s. 7.8 states/(s*actor), game length: 7.3\n","[2024-07-03 21:51:33.091] game_length_hist: [0, 0, 0, 0, 0, 129, 36, 136, 44, 215]\n","[2024-07-03 21:51:33.113] outcomes {'counts': [288, 80, 192], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-03 21:51:33.129] Buffer size: 16384. States seen: 28699\n","[2024-07-03 21:51:33.137] Loss. policy: 1.5844488143920898. value: 0.3980562116485089. l2reg: 0.07000889937626198. sum: 2.0525139254168607\n","[2024-07-03 21:51:33.155]\n","[2024-07-03 21:53:47.430] Losses(total: 2.004, policy: 1.557, value: 0.378, l2: 0.069)\n","[2024-07-03 21:53:47.440] Checkpoint saved: az-03.07.24-21:37/checkpoint--1\n","[2024-07-03 21:53:47.448] Step: 8\n","[2024-07-03 21:53:47.453] Collected  4102 states from 570 games, 31.3 states/s. 7.8 states/(s*actor), game length: 7.2\n","[2024-07-03 21:53:47.462] game_length_hist: [0, 0, 0, 0, 0, 140, 40, 150, 48, 192]\n","[2024-07-03 21:53:47.470] outcomes {'counts': [317, 88, 165], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-03 21:53:47.479] Buffer size: 16384. States seen: 32801\n","[2024-07-03 21:53:47.490] Loss. policy: 1.5568266427144408. value: 0.37774278107099235. l2reg: 0.06939490209333599. sum: 2.003964325878769\n","[2024-07-03 21:53:47.506]\n","[2024-07-03 21:55:57.073] Losses(total: 1.975, policy: 1.543, value: 0.363, l2: 0.070)\n","[2024-07-03 21:55:57.083] Checkpoint saved: az-03.07.24-21:37/checkpoint--1\n","[2024-07-03 21:55:57.091] Step: 9\n","[2024-07-03 21:55:57.101] Collected  4100 states from 554 games, 30.8 states/s. 7.7 states/(s*actor), game length: 7.4\n","[2024-07-03 21:55:57.105] game_length_hist: [0, 0, 0, 0, 0, 113, 29, 149, 49, 214]\n","[2024-07-03 21:55:57.116] outcomes {'counts': [289, 78, 187], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-03 21:55:57.121] Buffer size: 16384. States seen: 36901\n","[2024-07-03 21:55:57.134] Loss. policy: 1.5426498036831617. value: 0.36255594599060714. l2reg: 0.07017549645388499. sum: 1.9753812461276539\n","[2024-07-03 21:55:57.147]\n","[2024-07-03 21:58:12.303] Losses(total: 1.986, policy: 1.549, value: 0.366, l2: 0.070)\n","[2024-07-03 21:58:12.308] Checkpoint saved: az-03.07.24-21:37/checkpoint--1\n","[2024-07-03 21:58:12.313] Step: 10\n","[2024-07-03 21:58:12.318] Collected  4096 states from 569 games, 31.6 states/s. 7.9 states/(s*actor), game length: 7.2\n","[2024-07-03 21:58:12.322] game_length_hist: [0, 0, 0, 0, 0, 143, 43, 136, 52, 195]\n","[2024-07-03 21:58:12.337] outcomes {'counts': [307, 95, 167], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-03 21:58:12.343] Buffer size: 16384. States seen: 40997\n","[2024-07-03 21:58:12.361] Loss. policy: 1.5492680203169584. value: 0.3664910566294566. l2reg: 0.07004685892025009. sum: 1.985805935866665\n","[2024-07-03 21:58:12.362]\n","[2024-07-03 22:00:22.605] Losses(total: 1.977, policy: 1.546, value: 0.361, l2: 0.070)\n","[2024-07-03 22:00:22.616] Checkpoint saved: az-03.07.24-21:37/checkpoint--1\n","[2024-07-03 22:00:22.626] Step: 11\n","[2024-07-03 22:00:22.627] Collected  4096 states from 558 games, 30.2 states/s. 7.5 states/(s*actor), game length: 7.3\n","[2024-07-03 22:00:22.630] game_length_hist: [0, 0, 0, 0, 0, 123, 40, 137, 40, 218]\n","[2024-07-03 22:00:22.648] outcomes {'counts': [290, 80, 188], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-03 22:00:22.650] Buffer size: 16384. States seen: 45093\n","[2024-07-03 22:00:22.652] Loss. policy: 1.5458687031641603. value: 0.3608861507382244. l2reg: 0.07025374902877957. sum: 1.9770086029311642\n","[2024-07-03 22:00:22.662]\n","[2024-07-03 22:02:30.361] Losses(total: 1.957, policy: 1.546, value: 0.342, l2: 0.068)\n","[2024-07-03 22:02:30.369] Checkpoint saved: az-03.07.24-21:37/checkpoint--1\n","[2024-07-03 22:02:30.377] Step: 12\n","[2024-07-03 22:02:30.383] Collected  4098 states from 555 games, 31.4 states/s. 7.9 states/(s*actor), game length: 7.4\n","[2024-07-03 22:02:30.396] game_length_hist: [0, 0, 0, 0, 0, 118, 25, 152, 46, 214]\n","[2024-07-03 22:02:30.397] outcomes {'counts': [300, 71, 184], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-03 22:02:30.418] Buffer size: 16384. States seen: 49191\n","[2024-07-03 22:02:30.438] Loss. policy: 1.546212307177484. value: 0.34204582485836. l2reg: 0.06829543656203896. sum: 1.956553568597883\n","[2024-07-03 22:02:30.439]\n","[2024-07-03 22:04:42.866] Losses(total: 1.939, policy: 1.537, value: 0.335, l2: 0.067)\n","[2024-07-03 22:04:42.882] Checkpoint saved: az-03.07.24-21:37/checkpoint--1\n","[2024-07-03 22:04:42.890] Step: 13\n","[2024-07-03 22:04:42.898] Collected  4100 states from 559 games, 32.0 states/s. 8.0 states/(s*actor), game length: 7.3\n","[2024-07-03 22:04:42.924] game_length_hist: [0, 0, 0, 0, 0, 122, 27, 161, 40, 209]\n","[2024-07-03 22:04:42.954] outcomes {'counts': [311, 67, 181], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-03 22:04:42.966] Buffer size: 16384. States seen: 53291\n","[2024-07-03 22:04:42.970] Loss. policy: 1.537255048751831. value: 0.3346683826530352. l2reg: 0.06724216399015859. sum: 1.9391655953950249\n","[2024-07-03 22:04:42.973]\n","[2024-07-03 22:06:54.364] Losses(total: 1.936, policy: 1.540, value: 0.328, l2: 0.068)\n","[2024-07-03 22:06:54.374] Checkpoint saved: az-03.07.24-21:37/checkpoint--1\n","[2024-07-03 22:06:54.377] Step: 14\n","[2024-07-03 22:06:54.379] Collected  4103 states from 561 games, 30.9 states/s. 7.7 states/(s*actor), game length: 7.3\n","[2024-07-03 22:06:54.390] game_length_hist: [0, 0, 0, 0, 0, 119, 33, 160, 51, 198]\n","[2024-07-03 22:06:54.391] outcomes {'counts': [300, 84, 177], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-03 22:06:54.397] Buffer size: 16384. States seen: 57394\n","[2024-07-03 22:06:54.402] Loss. policy: 1.5400632284581661. value: 0.32828060048632324. l2reg: 0.06761735677719116. sum: 1.9359611857216805\n","[2024-07-03 22:06:54.411]\n","[2024-07-03 22:09:05.938] Losses(total: 1.913, policy: 1.527, value: 0.320, l2: 0.067)\n","[2024-07-03 22:09:05.965] Checkpoint saved: az-03.07.24-21:37/checkpoint--1\n","[2024-07-03 22:09:05.978] Step: 15\n","[2024-07-03 22:09:05.985] Collected  4100 states from 563 games, 31.4 states/s. 7.9 states/(s*actor), game length: 7.3\n","[2024-07-03 22:09:06.008] game_length_hist: [0, 0, 0, 0, 0, 133, 25, 156, 48, 201]\n","[2024-07-03 22:09:06.014] outcomes {'counts': [316, 73, 174], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-03 22:09:06.045] Buffer size: 16384. States seen: 61494\n","[2024-07-03 22:09:06.055] Loss. policy: 1.5268921162933111. value: 0.3200952758779749. l2reg: 0.06651224906090647. sum: 1.9134996412321925\n","[2024-07-03 22:09:06.069]\n","[2024-07-03 22:11:19.111] Losses(total: 1.885, policy: 1.520, value: 0.299, l2: 0.066)\n","[2024-07-03 22:11:19.119] Checkpoint saved: az-03.07.24-21:37/checkpoint--1\n","[2024-07-03 22:11:19.129] Step: 16\n","[2024-07-03 22:11:19.133] Collected  4102 states from 552 games, 30.9 states/s. 7.7 states/(s*actor), game length: 7.4\n","[2024-07-03 22:11:19.143] game_length_hist: [0, 0, 0, 0, 0, 121, 16, 151, 32, 232]\n","[2024-07-03 22:11:19.155] outcomes {'counts': [297, 48, 207], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-03 22:11:19.170] Buffer size: 16384. States seen: 65596\n","[2024-07-03 22:11:19.174] Loss. policy: 1.5201076595112681. value: 0.29893719393294305. l2reg: 0.06563667487353086. sum: 1.884681528317742\n","[2024-07-03 22:11:19.182]\n","[2024-07-03 22:13:25.563] Losses(total: 1.852, policy: 1.499, value: 0.289, l2: 0.064)\n","[2024-07-03 22:13:25.573] Checkpoint saved: az-03.07.24-21:37/checkpoint--1\n","[2024-07-03 22:13:25.577] Step: 17\n","[2024-07-03 22:13:25.580] Collected  4101 states from 550 games, 31.0 states/s. 7.7 states/(s*actor), game length: 7.5\n","[2024-07-03 22:13:25.586] game_length_hist: [0, 0, 0, 0, 0, 118, 18, 141, 41, 232]\n","[2024-07-03 22:13:25.592] outcomes {'counts': [276, 59, 215], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-03 22:13:25.615] Buffer size: 16384. States seen: 69697\n","[2024-07-03 22:13:25.629] Loss. policy: 1.499403627589345. value: 0.2894690928515047. l2reg: 0.06359676682041027. sum: 1.85246948726126\n","[2024-07-03 22:13:25.636]\n","[2024-07-03 22:15:39.618] Losses(total: 1.824, policy: 1.489, value: 0.275, l2: 0.061)\n","[2024-07-03 22:15:39.626] Checkpoint saved: az-03.07.24-21:37/checkpoint--1\n","[2024-07-03 22:15:39.630] Step: 18\n","[2024-07-03 22:15:39.640] Collected  4100 states from 542 games, 32.3 states/s. 8.1 states/(s*actor), game length: 7.6\n","[2024-07-03 22:15:39.651] game_length_hist: [0, 0, 0, 0, 0, 101, 13, 148, 39, 241]\n","[2024-07-03 22:15:39.661] outcomes {'counts': [266, 52, 224], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-03 22:15:39.670] Buffer size: 16384. States seen: 73797\n","[2024-07-03 22:15:39.674] Loss. policy: 1.4885645546019077. value: 0.2752181561663747. l2reg: 0.06051131506683305. sum: 1.8242940258351155\n","[2024-07-03 22:15:39.686]\n","[2024-07-03 22:17:51.016] Losses(total: 1.781, policy: 1.468, value: 0.255, l2: 0.058)\n","[2024-07-03 22:17:51.023] Checkpoint saved: az-03.07.24-21:37/checkpoint--1\n","[2024-07-03 22:17:51.027] Step: 19\n","[2024-07-03 22:17:51.031] Collected  4100 states from 535 games, 30.7 states/s. 7.7 states/(s*actor), game length: 7.7\n","[2024-07-03 22:17:51.038] game_length_hist: [0, 0, 0, 0, 0, 92, 18, 129, 35, 261]\n","[2024-07-03 22:17:51.049] outcomes {'counts': [241, 53, 241], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-03 22:17:51.054] Buffer size: 16384. States seen: 77897\n","[2024-07-03 22:17:51.063] Loss. policy: 1.467773082666099. value: 0.25548311648890376. l2reg: 0.05795599258271977. sum: 1.7812121917377226\n","[2024-07-03 22:17:51.072]\n","[2024-07-03 22:20:06.143] Losses(total: 1.782, policy: 1.463, value: 0.261, l2: 0.058)\n","[2024-07-03 22:20:06.158] Checkpoint saved: az-03.07.24-21:37/checkpoint--1\n","[2024-07-03 22:20:06.165] Step: 20\n","[2024-07-03 22:20:06.170] Collected  4098 states from 539 games, 31.2 states/s. 7.8 states/(s*actor), game length: 7.6\n","[2024-07-03 22:20:06.184] game_length_hist: [0, 0, 0, 0, 0, 94, 18, 140, 43, 244]\n","[2024-07-03 22:20:06.190] outcomes {'counts': [249, 61, 229], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-03 22:20:06.203] Buffer size: 16384. States seen: 81995\n","[2024-07-03 22:20:06.212] Loss. policy: 1.4627132415771484. value: 0.26076409593224525. l2reg: 0.05829667689977214. sum: 1.7817740144091658\n","[2024-07-03 22:20:06.217]\n","[2024-07-03 22:22:16.269] Losses(total: 1.761, policy: 1.456, value: 0.248, l2: 0.057)\n","[2024-07-03 22:22:16.282] Checkpoint saved: az-03.07.24-21:37/checkpoint--1\n","[2024-07-03 22:22:16.292] Step: 21\n","[2024-07-03 22:22:16.295] Collected  4096 states from 540 games, 30.3 states/s. 7.6 states/(s*actor), game length: 7.6\n","[2024-07-03 22:22:16.297] game_length_hist: [0, 0, 0, 0, 0, 101, 11, 145, 37, 246]\n","[2024-07-03 22:22:16.318] outcomes {'counts': [261, 48, 231], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-03 22:22:16.329] Buffer size: 16384. States seen: 86091\n","[2024-07-03 22:22:16.347] Loss. policy: 1.4560763519257307. value: 0.2482562354998663. l2reg: 0.057047569658607244. sum: 1.7613801570842043\n","[2024-07-03 22:22:16.355]\n","[2024-07-03 22:24:26.000] Losses(total: 1.744, policy: 1.443, value: 0.246, l2: 0.055)\n","[2024-07-03 22:24:26.005] Checkpoint saved: az-03.07.24-21:37/checkpoint--1\n","[2024-07-03 22:24:26.012] Step: 22\n","[2024-07-03 22:24:26.024] Collected  4104 states from 539 games, 31.5 states/s. 7.9 states/(s*actor), game length: 7.6\n","[2024-07-03 22:24:26.034] game_length_hist: [0, 0, 0, 0, 0, 96, 11, 147, 36, 249]\n","[2024-07-03 22:24:26.047] outcomes {'counts': [270, 47, 222], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-03 22:24:26.060] Buffer size: 16384. States seen: 90195\n","[2024-07-03 22:24:26.062] Loss. policy: 1.4428871786221862. value: 0.24618801556061953. l2reg: 0.05534669410553761. sum: 1.7444218882883433\n","[2024-07-03 22:24:26.070]\n","[2024-07-03 22:26:34.862] Losses(total: 1.749, policy: 1.451, value: 0.243, l2: 0.055)\n","[2024-07-03 22:26:34.876] Checkpoint saved: az-03.07.24-21:37/checkpoint--1\n","[2024-07-03 22:26:34.881] Step: 23\n","[2024-07-03 22:26:34.884] Collected  4097 states from 537 games, 31.7 states/s. 7.9 states/(s*actor), game length: 7.6\n","[2024-07-03 22:26:34.915] game_length_hist: [0, 0, 0, 0, 0, 100, 9, 143, 23, 262]\n","[2024-07-03 22:26:34.922] outcomes {'counts': [270, 32, 235], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-03 22:26:34.929] Buffer size: 16384. States seen: 94292\n","[2024-07-03 22:26:34.944] Loss. policy: 1.4509619614109397. value: 0.2433293677167967. l2reg: 0.05490643286611885. sum: 1.7491977619938552\n","[2024-07-03 22:26:34.952]\n","[2024-07-03 22:28:49.256] Losses(total: 1.746, policy: 1.452, value: 0.238, l2: 0.056)\n","[2024-07-03 22:28:49.265] Checkpoint saved: az-03.07.24-21:37/checkpoint--1\n","[2024-07-03 22:28:49.274] Step: 24\n","[2024-07-03 22:28:49.277] Collected  4099 states from 540 games, 31.9 states/s. 8.0 states/(s*actor), game length: 7.6\n","[2024-07-03 22:28:49.282] game_length_hist: [0, 0, 0, 0, 0, 101, 8, 156, 21, 254]\n","[2024-07-03 22:28:49.291] outcomes {'counts': [280, 29, 231], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-03 22:28:49.296] Buffer size: 16384. States seen: 98391\n","[2024-07-03 22:28:49.312] Loss. policy: 1.4517393000423908. value: 0.2376870969310403. l2reg: 0.05634202004875988. sum: 1.745768417022191\n","[2024-07-03 22:28:49.317]\n","[2024-07-03 22:30:59.065] Losses(total: 1.719, policy: 1.442, value: 0.221, l2: 0.056)\n","[2024-07-03 22:30:59.086] Checkpoint saved: az-03.07.24-21:37/checkpoint-25\n","[2024-07-03 22:30:59.094] Step: 25\n","[2024-07-03 22:30:59.099] Collected  4100 states from 541 games, 30.5 states/s. 7.6 states/(s*actor), game length: 7.6\n","[2024-07-03 22:30:59.113] game_length_hist: [0, 0, 0, 0, 0, 116, 6, 133, 21, 265]\n","[2024-07-03 22:30:59.129] outcomes {'counts': [269, 27, 245], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-03 22:30:59.175] Buffer size: 16384. States seen: 102491\n","[2024-07-03 22:30:59.189] Loss. policy: 1.4424193818122149. value: 0.22097987763118. l2reg: 0.055646511347731575. sum: 1.7190457707911264\n","[2024-07-03 22:30:59.203]\n","[2024-07-03 22:30:59.807] learner exiting\n","learner exiting\n","\n","actor-1 exitingactor-2 exiting\n","\n","\n","\n","actor-3 exiting\n","\n","actor-0 exiting\n","\n","evaluator-2 exiting\n","\n","evaluator-1 exiting\n","\n","evaluator-3 exiting\n","\n","evaluator-0 exiting\n","\n"]}],"source":["# –ù–ê CPU (50 –º–∏–Ω—É—Ç)\n","\n","alpha_zero(config)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":496058,"status":"ok","timestamp":1720251104395,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"},"user_tz":-180},"outputId":"7bd8ecac-f649-4bd9-81b4-4743677c5f5c","id":"4WcTjHWb2KQn"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Starting game tic_tac_toe\n","Writing logs and checkpoints to: az-06.07.24-07:01\n","Model type: resnet(128, 2)\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["actor-0 started\n","actor-1 started\n","actor-2 started\n","actor-3 started\n","evaluator-0 startedevaluator-1 started\n","\n","evaluator-2 startedlearner started\n","[2024-07-06 07:01:23.809] Initializing model\n","\n","evaluator-3 started\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"]},{"output_type":"stream","name":"stdout","text":["[2024-07-06 07:01:48.499] Model type: resnet(128, 2)\n","[2024-07-06 07:01:48.504] Model size: 597173 variables\n","[2024-07-06 07:01:52.839] Initial checkpoint: az-06.07.24-07:01/checkpoint-0\n","[2024-07-06 07:04:10.161] Losses(total: 2.274, policy: 1.626, value: 0.530, l2: 0.118)\n","[2024-07-06 07:04:10.182] Checkpoint saved: az-06.07.24-07:01/checkpoint--1\n","[2024-07-06 07:04:10.186] Step: 1\n","[2024-07-06 07:04:10.204] Collected  4102 states from 561 games, 26.3 states/s. 6.6 states/(s*actor), game length: 7.3\n","[2024-07-06 07:04:10.213] game_length_hist: [0, 0, 0, 0, 0, 136, 33, 118, 68, 206]\n","[2024-07-06 07:04:10.218] outcomes {'counts': [278, 101, 182], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-06 07:04:10.226] Buffer size: 4102. States seen: 4102\n","[2024-07-06 07:04:10.245]\n","[2024-07-06 07:04:25.080] Losses(total: 2.204, policy: 1.600, value: 0.470, l2: 0.134)\n","[2024-07-06 07:04:25.100] Checkpoint saved: az-06.07.24-07:01/checkpoint--1\n","[2024-07-06 07:04:25.106] Step: 2\n","[2024-07-06 07:04:25.113] Collected  4096 states from 551 games, 95.5 states/s. 23.9 states/(s*actor), game length: 7.4\n","[2024-07-06 07:04:25.149] game_length_hist: [0, 0, 0, 0, 0, 117, 32, 129, 41, 232]\n","[2024-07-06 07:04:25.154] outcomes {'counts': [282, 73, 196], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-06 07:04:25.173] Buffer size: 8198. States seen: 8198\n","[2024-07-06 07:04:25.176]\n","[2024-07-06 07:05:31.975] Losses(total: 2.159, policy: 1.609, value: 0.453, l2: 0.097)\n","[2024-07-06 07:05:31.982] Checkpoint saved: az-06.07.24-07:01/checkpoint--1\n","[2024-07-06 07:05:31.999] Step: 3\n","[2024-07-06 07:05:32.014] Collected  4103 states from 563 games, 70.0 states/s. 17.5 states/(s*actor), game length: 7.3\n","[2024-07-06 07:05:32.016] game_length_hist: [0, 0, 0, 0, 0, 138, 41, 128, 33, 223]\n","[2024-07-06 07:05:32.018] outcomes {'counts': [310, 74, 179], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-06 07:05:32.031] Buffer size: 12301. States seen: 12301\n","[2024-07-06 07:05:32.040]\n","[2024-07-06 07:06:59.007] Losses(total: 2.092, policy: 1.596, value: 0.424, l2: 0.072)\n","[2024-07-06 07:06:59.032] Checkpoint saved: az-06.07.24-07:01/checkpoint--1\n","[2024-07-06 07:06:59.043] Step: 4\n","[2024-07-06 07:06:59.072] Collected  4099 states from 561 games, 48.7 states/s. 12.2 states/(s*actor), game length: 7.3\n","[2024-07-06 07:06:59.084] game_length_hist: [0, 0, 0, 0, 0, 133, 37, 135, 37, 219]\n","[2024-07-06 07:06:59.117] outcomes {'counts': [305, 74, 182], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-06 07:06:59.128] Buffer size: 16384. States seen: 16400\n","[2024-07-06 07:06:59.151]\n","[2024-07-06 07:08:10.385] Losses(total: 2.069, policy: 1.595, value: 0.409, l2: 0.065)\n","[2024-07-06 07:08:10.419] Checkpoint saved: az-06.07.24-07:01/checkpoint--1\n","[2024-07-06 07:08:10.433] Step: 5\n","[2024-07-06 07:08:10.437] Collected  4103 states from 565 games, 57.5 states/s. 14.4 states/(s*actor), game length: 7.3\n","[2024-07-06 07:08:10.445] game_length_hist: [0, 0, 0, 0, 0, 127, 32, 167, 44, 195]\n","[2024-07-06 07:08:10.456] outcomes {'counts': [329, 76, 160], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-06 07:08:10.476] Buffer size: 16384. States seen: 20503\n","[2024-07-06 07:08:10.490]\n","[2024-07-06 07:09:30.042] Losses(total: 2.028, policy: 1.572, value: 0.385, l2: 0.071)\n","[2024-07-06 07:09:30.055] Checkpoint saved: az-06.07.24-07:01/checkpoint--1\n","[2024-07-06 07:09:30.064] Step: 6\n","[2024-07-06 07:09:30.074] Collected  4096 states from 552 games, 51.8 states/s. 13.0 states/(s*actor), game length: 7.4\n","[2024-07-06 07:09:30.079] game_length_hist: [0, 0, 0, 0, 0, 106, 32, 155, 42, 217]\n","[2024-07-06 07:09:30.091] outcomes {'counts': [290, 74, 188], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-06 07:09:30.102] Buffer size: 16384. States seen: 24599\n","[2024-07-06 07:09:30.111]\n","[2024-07-06 07:10:43.259] Losses(total: 1.977, policy: 1.553, value: 0.352, l2: 0.072)\n","[2024-07-06 07:10:43.283] Checkpoint saved: az-06.07.24-07:01/checkpoint--1\n","[2024-07-06 07:10:43.293] Step: 7\n","[2024-07-06 07:10:43.298] Collected  4101 states from 544 games, 60.2 states/s. 15.1 states/(s*actor), game length: 7.5\n","[2024-07-06 07:10:43.312] game_length_hist: [0, 0, 0, 0, 0, 94, 16, 168, 35, 231]\n","[2024-07-06 07:10:43.315] outcomes {'counts': [291, 51, 202], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-06 07:10:43.331] Buffer size: 16384. States seen: 28700\n","[2024-07-06 07:10:43.333]\n","[2024-07-06 07:11:57.043] Losses(total: 1.944, policy: 1.544, value: 0.327, l2: 0.073)\n","[2024-07-06 07:11:57.057] Checkpoint saved: az-06.07.24-07:01/checkpoint--1\n","[2024-07-06 07:11:57.065] Step: 8\n","[2024-07-06 07:11:57.083] Collected  4097 states from 541 games, 53.2 states/s. 13.3 states/(s*actor), game length: 7.6\n","[2024-07-06 07:11:57.128] game_length_hist: [0, 0, 0, 0, 0, 95, 23, 152, 19, 252]\n","[2024-07-06 07:11:57.156] outcomes {'counts': [278, 42, 221], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-06 07:11:57.162] Buffer size: 16384. States seen: 32797\n","[2024-07-06 07:11:57.190]\n","[2024-07-06 07:13:10.215] Losses(total: 1.886, policy: 1.516, value: 0.297, l2: 0.073)\n","[2024-07-06 07:13:10.227] Checkpoint saved: az-06.07.24-07:01/checkpoint--1\n","[2024-07-06 07:13:10.232] Step: 9\n","[2024-07-06 07:13:10.240] Collected  4096 states from 540 games, 55.4 states/s. 13.9 states/(s*actor), game length: 7.6\n","[2024-07-06 07:13:10.253] game_length_hist: [0, 0, 0, 0, 0, 96, 8, 166, 24, 246]\n","[2024-07-06 07:13:10.277] outcomes {'counts': [291, 32, 217], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-06 07:13:10.280] Buffer size: 16384. States seen: 36893\n","[2024-07-06 07:13:10.311]\n","[2024-07-06 07:14:21.545] Losses(total: 1.820, policy: 1.492, value: 0.257, l2: 0.071)\n","[2024-07-06 07:14:21.552] Checkpoint saved: az-06.07.24-07:01/checkpoint--1\n","[2024-07-06 07:14:21.559] Step: 10\n","[2024-07-06 07:14:21.562] Collected  4100 states from 536 games, 56.1 states/s. 14.0 states/(s*actor), game length: 7.6\n","[2024-07-06 07:14:21.576] game_length_hist: [0, 0, 0, 0, 0, 91, 15, 150, 15, 265]\n","[2024-07-06 07:14:21.583] outcomes {'counts': [261, 30, 245], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-06 07:14:21.588] Buffer size: 16384. States seen: 40993\n","[2024-07-06 07:14:21.590]\n","[2024-07-06 07:15:29.178] Losses(total: 1.773, policy: 1.473, value: 0.234, l2: 0.067)\n","[2024-07-06 07:15:29.182] Checkpoint saved: az-06.07.24-07:01/checkpoint--1\n","[2024-07-06 07:15:29.205] Step: 11\n","[2024-07-06 07:15:29.210] Collected  4100 states from 540 games, 61.8 states/s. 15.4 states/(s*actor), game length: 7.6\n","[2024-07-06 07:15:29.213] game_length_hist: [0, 0, 0, 0, 0, 111, 14, 129, 16, 270]\n","[2024-07-06 07:15:29.273] outcomes {'counts': [256, 30, 254], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-06 07:15:29.280] Buffer size: 16384. States seen: 45093\n","[2024-07-06 07:15:29.293]\n","[2024-07-06 07:16:42.371] Losses(total: 1.735, policy: 1.453, value: 0.217, l2: 0.064)\n","[2024-07-06 07:16:42.382] Checkpoint saved: az-06.07.24-07:01/checkpoint--1\n","[2024-07-06 07:16:42.387] Step: 12\n","[2024-07-06 07:16:42.391] Collected  4098 states from 527 games, 55.2 states/s. 13.8 states/(s*actor), game length: 7.8\n","[2024-07-06 07:16:42.403] game_length_hist: [0, 0, 0, 0, 0, 95, 4, 118, 17, 293]\n","[2024-07-06 07:16:42.406] outcomes {'counts': [227, 21, 279], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-06 07:16:42.415] Buffer size: 16384. States seen: 49191\n","[2024-07-06 07:16:42.417]\n","[2024-07-06 07:17:56.260] Losses(total: 1.708, policy: 1.437, value: 0.207, l2: 0.063)\n","[2024-07-06 07:17:56.264] Checkpoint saved: az-06.07.24-07:01/checkpoint--1\n","[2024-07-06 07:17:56.280] Step: 13\n","[2024-07-06 07:17:56.287] Collected  4097 states from 529 games, 56.1 states/s. 14.0 states/(s*actor), game length: 7.7\n","[2024-07-06 07:17:56.290] game_length_hist: [0, 0, 0, 0, 0, 97, 2, 125, 20, 285]\n","[2024-07-06 07:17:56.300] outcomes {'counts': [237, 22, 270], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-06 07:17:56.329] Buffer size: 16384. States seen: 53288\n","[2024-07-06 07:17:56.332]\n","[2024-07-06 07:19:06.925] Losses(total: 1.693, policy: 1.428, value: 0.204, l2: 0.061)\n","[2024-07-06 07:19:06.937] Checkpoint saved: az-06.07.24-07:01/checkpoint--1\n","[2024-07-06 07:19:06.941] Step: 14\n","[2024-07-06 07:19:06.949] Collected  4096 states from 530 games, 60.6 states/s. 15.2 states/(s*actor), game length: 7.7\n","[2024-07-06 07:19:06.963] game_length_hist: [0, 0, 0, 0, 0, 96, 7, 124, 21, 282]\n","[2024-07-06 07:19:07.016] outcomes {'counts': [235, 28, 267], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-06 07:19:07.019] Buffer size: 16384. States seen: 57384\n","[2024-07-06 07:19:07.022]\n","[2024-07-06 07:20:16.777] Losses(total: 1.680, policy: 1.418, value: 0.202, l2: 0.060)\n","[2024-07-06 07:20:16.786] Checkpoint saved: az-06.07.24-07:01/checkpoint--1\n","[2024-07-06 07:20:16.798] Step: 15\n","[2024-07-06 07:20:16.802] Collected  4097 states from 523 games, 54.8 states/s. 13.7 states/(s*actor), game length: 7.8\n","[2024-07-06 07:20:16.818] game_length_hist: [0, 0, 0, 0, 0, 88, 1, 116, 23, 295]\n","[2024-07-06 07:20:16.830] outcomes {'counts': [216, 24, 283], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-06 07:20:16.839] Buffer size: 16384. States seen: 61481\n","[2024-07-06 07:20:16.844]\n","[2024-07-06 07:21:30.170] Losses(total: 1.676, policy: 1.415, value: 0.201, l2: 0.060)\n","[2024-07-06 07:21:30.179] Checkpoint saved: az-06.07.24-07:01/checkpoint--1\n","[2024-07-06 07:21:30.190] Step: 16\n","[2024-07-06 07:21:30.191] Collected  4098 states from 527 games, 60.1 states/s. 15.0 states/(s*actor), game length: 7.8\n","[2024-07-06 07:21:30.200] game_length_hist: [0, 0, 0, 0, 0, 80, 20, 125, 15, 287]\n","[2024-07-06 07:21:30.221] outcomes {'counts': [224, 35, 268], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-06 07:21:30.222] Buffer size: 16384. States seen: 65579\n","[2024-07-06 07:21:30.279]\n","[2024-07-06 07:22:37.581] Losses(total: 1.657, policy: 1.400, value: 0.198, l2: 0.059)\n","[2024-07-06 07:22:37.594] Checkpoint saved: az-06.07.24-07:01/checkpoint--1\n","[2024-07-06 07:22:37.600] Step: 17\n","[2024-07-06 07:22:37.604] Collected  4102 states from 527 games, 56.1 states/s. 14.0 states/(s*actor), game length: 7.8\n","[2024-07-06 07:22:37.605] game_length_hist: [0, 0, 0, 0, 0, 86, 8, 128, 17, 288]\n","[2024-07-06 07:22:37.615] outcomes {'counts': [228, 25, 274], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-06 07:22:37.628] Buffer size: 16384. States seen: 69681\n","[2024-07-06 07:22:37.637]\n","[2024-07-06 07:23:49.337] Losses(total: 1.650, policy: 1.395, value: 0.197, l2: 0.059)\n","[2024-07-06 07:23:49.340] Checkpoint saved: az-06.07.24-07:01/checkpoint--1\n","[2024-07-06 07:23:49.350] Step: 18\n","[2024-07-06 07:23:49.360] Collected  4103 states from 518 games, 60.0 states/s. 15.0 states/(s*actor), game length: 7.9\n","[2024-07-06 07:23:49.361] game_length_hist: [0, 0, 0, 0, 0, 67, 14, 116, 17, 304]\n","[2024-07-06 07:23:49.377] outcomes {'counts': [199, 31, 288], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-06 07:23:49.380] Buffer size: 16384. States seen: 73784\n","[2024-07-06 07:23:49.385]\n","[2024-07-06 07:24:56.079] Checkpoint saved: az-06.07.24-07:01/checkpoint--1\n","[2024-07-06 07:24:56.095] Step: 19\n","[2024-07-06 07:24:56.097] Collected  4099 states from 518 games, 60.0 states/s. 15.0 states/(s*actor), game length: 7.9\n","[2024-07-06 07:24:56.110] game_length_hist: [0, 0, 0, 0, 0, 77, 16, 98, 11, 316]\n","[2024-07-06 07:24:56.115] outcomes {'counts': [198, 27, 293], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-06 07:24:56.116] Buffer size: 16384. States seen: 77883\n","[2024-07-06 07:24:56.135]\n","[2024-07-06 07:26:06.183] Losses(total: 1.625, policy: 1.376, value: 0.192, l2: 0.057)\n","[2024-07-06 07:26:06.194] Checkpoint saved: az-06.07.24-07:01/checkpoint--1\n","[2024-07-06 07:26:06.203] Step: 20\n","[2024-07-06 07:26:06.213] Collected  4099 states from 527 games, 60.1 states/s. 15.0 states/(s*actor), game length: 7.8\n","[2024-07-06 07:26:06.227] game_length_hist: [0, 0, 0, 0, 0, 77, 24, 124, 16, 286]\n","[2024-07-06 07:26:06.244] outcomes {'counts': [213, 40, 274], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-06 07:26:06.249] Buffer size: 16384. States seen: 81982\n","[2024-07-06 07:26:06.252]\n","[2024-07-06 07:27:05.424] Losses(total: 1.598, policy: 1.359, value: 0.182, l2: 0.057)\n","[2024-07-06 07:27:05.437] Checkpoint saved: az-06.07.24-07:01/checkpoint--1\n","[2024-07-06 07:27:05.441] Step: 21\n","[2024-07-06 07:27:05.444] Collected  4103 states from 509 games, 67.6 states/s. 16.9 states/(s*actor), game length: 8.1\n","[2024-07-06 07:27:05.446] game_length_hist: [0, 0, 0, 0, 0, 62, 6, 104, 4, 333]\n","[2024-07-06 07:27:05.452] outcomes {'counts': [177, 10, 322], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-06 07:27:05.463] Buffer size: 16384. States seen: 86085\n","[2024-07-06 07:27:05.464]\n","[2024-07-06 07:28:11.702] Losses(total: 1.584, policy: 1.354, value: 0.175, l2: 0.055)\n","[2024-07-06 07:28:11.709] Checkpoint saved: az-06.07.24-07:01/checkpoint--1\n","[2024-07-06 07:28:11.716] Step: 22\n","[2024-07-06 07:28:11.719] Collected  4096 states from 519 games, 58.8 states/s. 14.7 states/(s*actor), game length: 7.9\n","[2024-07-06 07:28:11.723] game_length_hist: [0, 0, 0, 0, 0, 76, 10, 112, 17, 304]\n","[2024-07-06 07:28:11.724] outcomes {'counts': [200, 27, 292], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-06 07:28:11.726] Buffer size: 16384. States seen: 90181\n","[2024-07-06 07:28:11.766]\n","[2024-07-06 07:29:20.032] Losses(total: 1.603, policy: 1.364, value: 0.185, l2: 0.054)\n","[2024-07-06 07:29:20.056] Checkpoint saved: az-06.07.24-07:01/checkpoint--1\n","[2024-07-06 07:29:20.061] Step: 23\n","[2024-07-06 07:29:20.069] Collected  4102 states from 515 games, 63.6 states/s. 15.9 states/(s*actor), game length: 8.0\n","[2024-07-06 07:29:20.072] game_length_hist: [0, 0, 0, 0, 0, 63, 12, 108, 29, 303]\n","[2024-07-06 07:29:20.082] outcomes {'counts': [185, 41, 289], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-06 07:29:20.093] Buffer size: 16384. States seen: 94283\n","[2024-07-06 07:29:20.109]\n","[2024-07-06 07:30:26.990] Losses(total: 1.596, policy: 1.361, value: 0.180, l2: 0.054)\n","[2024-07-06 07:30:27.001] Checkpoint saved: az-06.07.24-07:01/checkpoint--1\n","[2024-07-06 07:30:27.003] Step: 24\n","[2024-07-06 07:30:27.007] Collected  4103 states from 521 games, 61.6 states/s. 15.4 states/(s*actor), game length: 7.9\n","[2024-07-06 07:30:27.013] game_length_hist: [0, 0, 0, 0, 0, 77, 9, 117, 17, 301]\n","[2024-07-06 07:30:27.022] outcomes {'counts': [218, 26, 277], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-06 07:30:27.029] Buffer size: 16384. States seen: 98386\n","[2024-07-06 07:30:27.032]\n","[2024-07-06 07:31:37.982] Losses(total: 1.614, policy: 1.375, value: 0.186, l2: 0.054)\n","[2024-07-06 07:31:38.036] Checkpoint saved: az-06.07.24-07:01/checkpoint-25\n","[2024-07-06 07:31:38.042] Step: 25\n","[2024-07-06 07:31:38.046] Collected  4102 states from 515 games, 57.1 states/s. 14.3 states/(s*actor), game length: 8.0\n","[2024-07-06 07:31:38.052] game_length_hist: [0, 0, 0, 0, 0, 60, 7, 129, 14, 305]\n","[2024-07-06 07:31:38.069] outcomes {'counts': [199, 21, 295], 'names': ['Player1', 'Player2', 'Draw']}\n","[2024-07-06 07:31:38.071] Buffer size: 16384. States seen: 102488\n","[2024-07-06 07:31:38.092]\n","[2024-07-06 07:31:38.759] learner exiting\n","learner exiting\n","\n","actor-2 exiting\n","\n","actor-0 exiting\n","\n","actor-1 exitingactor-3 exiting\n","\n","\n","\n","evaluator-1 exiting\n","\n","evaluator-0 exiting\n","\n","evaluator-2 exiting\n","\n","evaluator-3 exiting\n","\n"]}],"source":["# –ù–ê GPU (30 –º–∏–Ω—É—Ç)\n","\n","alpha_zero(config)"]},{"cell_type":"markdown","source":["##–ò–≥—Ä–∞ —Å –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é (CPU)"],"metadata":{"id":"5ETKq1oIvaza"}},{"cell_type":"code","source":["game = pyspiel.load_game(\"tic_tac_toe\")\n","config = config._replace(\n","    observation_shape=game.observation_tensor_shape(),\n","    output_size=game.num_distinct_actions())"],"metadata":{"id":"XBxo8JJGvdsE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FnVbMKXimkVx","executionInfo":{"status":"ok","timestamp":1720094182346,"user_tz":-180,"elapsed":491,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"}},"outputId":"c8c1439a-dfff-4c33-c333-47c45c6b4fce"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Config(game='tic_tac_toe', path='az-04.07.24-11:46', learning_rate=0.01, weight_decay=0.0001, train_batch_size=128, replay_buffer_size=16384, replay_buffer_reuse=4, max_steps=25, checkpoint_freq=25, actors=4, evaluators=4, evaluation_window=50, eval_levels=7, uct_c=1, max_simulations=20, policy_alpha=0.25, policy_epsilon=1, temperature=1, temperature_drop=4, nn_model='resnet', nn_width=128, nn_depth=2, observation_shape=[3, 3, 3], output_size=9, quiet=True)"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["model = _init_model_from_config(config)\n","model.load_checkpoint(\"/content/drive/MyDrive/az-03.07.24-21:37/checkpoint-25\")\n","\n","az_evaluator = AlphaZeroEvaluator(model)\n","az = _init_bot(config, game, az_evaluator, True)"],"metadata":{"id":"aRTRw2q3yCeU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["state = game.new_initial_state()"],"metadata":{"id":"B04sUW7Zztdu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def print_stats(root):\n","    print(\"action \\t  reward       outcome\\t explore_count\")\n","    for child in root.children:\n","        mean_reward = child.total_reward / child.explore_count if child.explore_count > 0 else 0\n","        print(f\"{child.action}\\t {round(mean_reward, 5):7}\\t {child.outcome}\\t {child.explore_count}\")"],"metadata":{"id":"fwqSbEz7kjtm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# –•–æ–¥ —Å–æ–ø–µ—Ä–Ω–∏–∫–∞\n","root = az.mcts_search(state)\n","print_stats(root)\n","state.apply_action(root.best_child().action)\n","state"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720095143444,"user_tz":-180,"elapsed":391,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"}},"outputId":"cc07cdba-3b22-48d4-c395-e82326a351da","id":"gaYWAeDxkkJR"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["action \t  reward       outcome\t explore_count\n","6\t       0\t None\t 0\n","4\t  0.4174\t None\t 11\n","5\t       0\t None\t 0\n","3\t       0\t None\t 0\n","2\t 0.47469\t None\t 7\n","1\t       0\t None\t 0\n","0\t 0.39978\t None\t 1\n","8\t       0\t None\t 0\n","7\t       0\t None\t 0\n"]},{"output_type":"execute_result","data":{"text/plain":["...\n",".x.\n","..."]},"metadata":{},"execution_count":61}]},{"cell_type":"code","source":["# –ú–æ–π —Ö–æ–¥\n","print_stats(az.mcts_search(state))\n","state.apply_action(0)\n","state"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"je0qkQFMy3GU","executionInfo":{"status":"ok","timestamp":1720095172189,"user_tz":-180,"elapsed":360,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"}},"outputId":"b1bec9ce-8542-4bee-e5c5-38cbf2b18fc5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["action \t  reward       outcome\t explore_count\n","0\t -0.14019\t None\t 5\n","7\t -0.6125\t None\t 1\n","3\t -0.79627\t None\t 1\n","8\t -0.23917\t None\t 2\n","5\t -0.71531\t None\t 1\n","2\t -0.21865\t None\t 5\n","6\t -0.10097\t None\t 3\n","1\t -0.4682\t None\t 1\n"]},{"output_type":"execute_result","data":{"text/plain":["o..\n",".x.\n","..."]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["# –•–æ–¥ —Å–æ–ø–µ—Ä–Ω–∏–∫–∞\n","root = az.mcts_search(state)\n","print_stats(root)\n","state.apply_action(root.best_child().action)\n","state"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FqYWnzIjyX8c","executionInfo":{"status":"ok","timestamp":1720095248621,"user_tz":-180,"elapsed":953,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"}},"outputId":"828a8105-35f7-4c25-c2f6-a9edb8d955ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["action \t  reward       outcome\t explore_count\n","5\t       0\t None\t 0\n","2\t -0.02314\t None\t 1\n","3\t 0.19252\t None\t 2\n","6\t 0.06331\t None\t 2\n","7\t  0.3599\t None\t 12\n","8\t       0\t None\t 0\n","1\t 0.12811\t None\t 2\n"]},{"output_type":"execute_result","data":{"text/plain":["o..\n",".x.\n",".x."]},"metadata":{},"execution_count":63}]},{"cell_type":"code","source":["# –ú–æ–π —Ö–æ–¥\n","print_stats(az.mcts_search(state))\n","state.apply_action(1)\n","state"],"metadata":{"executionInfo":{"status":"ok","timestamp":1720095514564,"user_tz":-180,"elapsed":8,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"}},"outputId":"935ebeb0-ef1e-4005-d29d-b2804664c946","colab":{"base_uri":"https://localhost:8080/"},"id":"f2yAMjxfy8lm"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["action \t  reward       outcome\t explore_count\n","1\t  0.0342\t None\t 14\n","6\t -0.74291\t None\t 1\n","2\t -0.88804\t None\t 1\n","5\t -0.84577\t None\t 1\n","8\t -0.98804\t None\t 1\n","3\t -0.93622\t None\t 1\n"]},{"output_type":"execute_result","data":{"text/plain":["oo.\n",".x.\n",".x."]},"metadata":{},"execution_count":64}]},{"cell_type":"code","source":["# –•–æ–¥ —Å–æ–ø–µ—Ä–Ω–∏–∫–∞\n","root = az.mcts_search(state)\n","print_stats(root)\n","state.apply_action(root.best_child().action)\n","state"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720095552513,"user_tz":-180,"elapsed":2615,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"}},"outputId":"c49dbcbb-2b55-48ed-a0f2-516860d37bd5","id":"-FGb8vYSy8ll"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["action \t  reward       outcome\t explore_count\n","8\t -0.51336\t None\t 1\n","6\t -0.58918\t None\t 1\n","5\t -0.48038\t None\t 2\n","2\t 0.03918\t None\t 14\n","3\t -0.01541\t None\t 1\n"]},{"output_type":"execute_result","data":{"text/plain":["oox\n",".x.\n",".x."]},"metadata":{},"execution_count":65}]},{"cell_type":"code","source":["# –ú–æ–π —Ö–æ–¥\n","print_stats(az.mcts_search(state))\n","state.apply_action(6)\n","state"],"metadata":{"executionInfo":{"status":"ok","timestamp":1720095605831,"user_tz":-180,"elapsed":635,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"}},"outputId":"9f07410c-14e1-4e0b-965c-b8df38aa9685","colab":{"base_uri":"https://localhost:8080/"},"id":"MZ86_pxgzBgE"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["action \t  reward       outcome\t explore_count\n","8\t -0.73398\t None\t 1\n","3\t -0.87934\t None\t 1\n","5\t -0.22724\t None\t 1\n","6\t 0.03032\t None\t 16\n"]},{"output_type":"execute_result","data":{"text/plain":["oox\n",".x.\n","ox."]},"metadata":{},"execution_count":67}]},{"cell_type":"code","source":["# –•–æ–¥ —Å–æ–ø–µ—Ä–Ω–∏–∫–∞\n","root = az.mcts_search(state)\n","print_stats(root)\n","state.apply_action(root.best_child().action)\n","state"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720095626770,"user_tz":-180,"elapsed":684,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"}},"outputId":"48ba76bc-63c7-495a-936a-3e65faaaed8e","id":"8IX-V6JkzBf9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["action \t  reward       outcome\t explore_count\n","5\t -0.51312\t None\t 2\n","8\t -0.17318\t None\t 1\n","3\t 0.04609\t None\t 16\n"]},{"output_type":"execute_result","data":{"text/plain":["oox\n","xx.\n","ox."]},"metadata":{},"execution_count":68}]},{"cell_type":"code","source":["# –ú–æ–π —Ö–æ–¥\n","print_stats(az.mcts_search(state))\n","state.apply_action(5)\n","state"],"metadata":{"executionInfo":{"status":"ok","timestamp":1720095663103,"user_tz":-180,"elapsed":409,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"}},"outputId":"83864b85-c90b-4cee-cb57-d0ab8153db3c","colab":{"base_uri":"https://localhost:8080/"},"id":"3Q5eviuzzGmW"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["action \t  reward       outcome\t explore_count\n","8\t -0.78368\t None\t 1\n","5\t 0.00129\t None\t 18\n"]},{"output_type":"execute_result","data":{"text/plain":["oox\n","xxo\n","ox."]},"metadata":{},"execution_count":70}]},{"cell_type":"code","source":["# –•–æ–¥ —Å–æ–ø–µ—Ä–Ω–∏–∫–∞\n","root = az.mcts_search(state)\n","print_stats(root)\n","state.apply_action(root.best_child().action)\n","state"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720095676681,"user_tz":-180,"elapsed":2001,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"}},"outputId":"cac9699c-999c-49ac-9c07-eb64c46085be","id":"lk9hObTPzGmJ"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["action \t  reward       outcome\t explore_count\n","8\t     0.0\t [0.0, 0.0]\t 19\n"]},{"output_type":"execute_result","data":{"text/plain":["oox\n","xxo\n","oxx"]},"metadata":{},"execution_count":71}]},{"cell_type":"code","source":[],"metadata":{"id":"hRkb9dOUs7SU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UpSFlijSs7Vg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["state = game.new_initial_state()"],"metadata":{"id":"pLA6M8o8s7hn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["root = SearchNode(None, state.current_player(), 1)"],"metadata":{"id":"LuPFvDzYtK8-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["visit_path, working_state = az._apply_tree_policy(root, state)"],"metadata":{"id":"9_Qe9Aa-s856"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["visit_path"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tKbgSBE5tM5w","executionInfo":{"status":"ok","timestamp":1720095928424,"user_tz":-180,"elapsed":577,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"}},"outputId":"3e4c759a-0d1b-41b7-d30c-97a6ca743f9b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<__main__.SearchNode at 0x7c5e8a2141c0>]"]},"metadata":{},"execution_count":76}]},{"cell_type":"code","source":["az.evaluator.evaluate(working_state)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9DdofesysQ8S","executionInfo":{"status":"ok","timestamp":1720096020408,"user_tz":-180,"elapsed":353,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"}},"outputId":"d09324ee-cf09-4ffb-ead5-26423856a68e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.40663803, -0.40663803], dtype=float32)"]},"metadata":{},"execution_count":78}]},{"cell_type":"code","source":[],"metadata":{"id":"0TZrEvFdtRFg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sh9y5m1egpOi"},"source":["# AlphaZero (OLD)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XJDFcjEpSsZg","colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"status":"ok","timestamp":1719177138619,"user_tz":-180,"elapsed":6,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"}},"outputId":"eb92af8a-b821-4dea-9b58-0d82622bea20"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nThis implements the AlphaZero training algorithm. It spawns N actors which feed\\ntrajectories into a replay buffer which are consumed by a learner. The learner\\ngenerates new weights, saves a checkpoint, and tells the actors to update. There\\nare also M evaluators running games continuously against a standard MCTS+Solver,\\nthough each at a different difficulty (ie number of simulations for MCTS).\\n\\nDue to the multi-process nature of this algorithm the logs are written to files,\\none per process. The learner logs are also output to stdout. The checkpoints are\\nalso written to the same directory.\\n\\nLinks to relevant articles/papers:\\n  https://deepmind.com/blog/article/alphago-zero-starting-scratch has an open\\n    access link to the AlphaGo Zero nature paper.\\n  https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go\\n    has an open access link to the AlphaZero science paper.\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}],"source":["\"\"\"\n","This implements the AlphaZero training algorithm. It spawns N actors which feed\n","trajectories into a replay buffer which are consumed by a learner. The learner\n","generates new weights, saves a checkpoint, and tells the actors to update. There\n","are also M evaluators running games continuously against a standard MCTS+Solver,\n","though each at a different difficulty (ie number of simulations for MCTS).\n","\n","Due to the multi-process nature of this algorithm the logs are written to files,\n","one per process. The learner logs are also output to stdout. The checkpoints are\n","also written to the same directory.\n","\n","Links to relevant articles/papers:\n","  https://deepmind.com/blog/article/alphago-zero-starting-scratch has an open\n","    access link to the AlphaGo Zero nature paper.\n","  https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go\n","    has an open access link to the AlphaZero science paper.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8493,"status":"ok","timestamp":1719177161460,"user":{"displayName":"–ú–∞–∫—Å–∏–º –¢–∏–º–æ—à–∫–∏–Ω","userId":"09734858170620036336"},"user_tz":-180},"id":"WW74HoyfSuN5","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2d0b5188-60bd-4fc3-f2af-6287b4dff9ed"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/open_spiel/python/algorithms/alpha_zero/model.py:27: UserWarning: Python AlphaZero has known issues when using Keras 3 and may be removed in a future version unless fixed. See OpenSpiel github issue #1206 for details.\n","  warnings.warn(\n"]}],"source":["import collections\n","import datetime\n","import functools\n","import itertools\n","import json\n","import os\n","import random\n","import sys\n","import tempfile\n","import time\n","import traceback\n","\n","import pyspiel\n","from open_spiel.python.utils import stats\n","from open_spiel.python.utils import spawn\n","from open_spiel.python.algorithms.alpha_zero import model as model_lib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JS3t5yeluqKz"},"outputs":[],"source":["\"\"\"An MCTS Evaluator for an AlphaZero model.\"\"\"\n","\n","from open_spiel.python.utils import lru_cache\n","\n","class AlphaZeroEvaluator(object):\n","    def __init__(self, model, cache_size=2**16):\n","        self._model = model\n","        self._cache = lru_cache.LRUCache(cache_size)\n","\n","    def cache_info(self):\n","        return self._cache.info()\n","\n","    def clear_cache(self):\n","        self._cache.clear()\n","\n","    def _inference(self, state):\n","        # Make a singleton batch\n","        obs = np.expand_dims(state.observation_tensor(), 0)\n","        mask = np.expand_dims(state.legal_actions_mask(), 0)\n","\n","        # ndarray isn't hashable\n","        cache_key = obs.tobytes() + mask.tobytes()\n","\n","        value, policy = self._cache.make(cache_key, lambda: self._model.inference(obs, mask))\n","\n","        return value[0, 0], policy[0]  # Unpack batch\n","\n","    # Returns a value for the given state\n","    def evaluate(self, state):\n","        value, _ = self._inference(state)\n","        return np.array([value, -value])\n","\n","    def prior(self, state):\n","        # Returns the probabilities for all actions.\n","        _, policy = self._inference(state)\n","        return [(action, policy[action]) for action in state.legal_actions()]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ywGsa6uS7HK"},"outputs":[],"source":["# A particular point along a trajectory\n","class TrajectoryState(object):\n","    def __init__(self, observation, current_player, legals_mask, action, policy, value):\n","        self.observation = observation\n","        self.current_player = current_player\n","        self.legals_mask = legals_mask\n","        self.action = action\n","        self.policy = policy\n","        self.value = value"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4pUOnkVQTEJB"},"outputs":[],"source":["# A sequence of observations, actions and policies, and the outcomes\n","class Trajectory(object):\n","    def __init__(self):\n","        self.states = []\n","        self.returns = None\n","\n","    def add(self, information_state, action, policy):\n","        self.states.append((information_state, action, policy))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"StWItEefTLH4"},"outputs":[],"source":["# A fixed size buffer that keeps the newest values\n","class Buffer(object):\n","    def __init__(self, max_size):\n","        self.max_size = max_size\n","        self.data = []\n","        self.total_seen = 0  # The number of items that have passed through.\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __bool__(self):\n","        return bool(self.data)\n","\n","    def append(self, val):\n","        return self.extend([val])\n","\n","    def extend(self, batch):\n","        batch = list(batch)\n","        self.total_seen += len(batch)\n","        self.data.extend(batch)\n","        self.data[:-self.max_size] = []\n","\n","    def sample(self, count):\n","        return random.sample(self.data, count)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hJrpAoj0eVWQ"},"outputs":[],"source":["# A config for the model/experiment\n","class Config(collections.namedtuple(\n","    \"Config\", [\n","        \"game\",\n","        \"path\",\n","        \"learning_rate\",\n","        \"weight_decay\",\n","        \"train_batch_size\",\n","        \"replay_buffer_size\",\n","        \"replay_buffer_reuse\",\n","        \"max_steps\",\n","        \"checkpoint_freq\",\n","        \"actors\",\n","        \"evaluators\",\n","        \"evaluation_window\",\n","        \"eval_levels\",\n","\n","        \"uct_c\",\n","        \"max_simulations\",\n","        \"policy_alpha\",\n","        \"policy_epsilon\",\n","        \"temperature\",\n","        \"temperature_drop\",\n","\n","        \"nn_model\",\n","        \"nn_width\",\n","        \"nn_depth\",\n","        \"observation_shape\",\n","        \"output_size\",\n","\n","        \"quiet\",\n","    ])):\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qj4AUOtYhJRO"},"outputs":[],"source":["def _init_model_from_config(config):\n","  return model_lib.Model.build_model(\n","      config.nn_model,\n","      config.observation_shape,\n","      config.output_size,\n","      config.nn_width,\n","      config.nn_depth,\n","      config.weight_decay,\n","      config.learning_rate,\n","      config.path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6nNu08eBhn_n"},"outputs":[],"source":["import datetime\n","import os\n","\n","# A logger to print stuff to a file\n","class FileLogger(object):\n","\n","  def __init__(self, path, name, quiet=False, also_to_stdout=False):\n","    self._fd = open(os.path.join(path, \"log-{}.txt\".format(name)), \"w\")\n","    self._quiet = quiet\n","    self.also_to_stdout = also_to_stdout\n","\n","  def print(self, *args):\n","    # Date/time with millisecond precision.\n","    date_prefix = \"[{}]\".format(datetime.datetime.now().isoformat(\" \")[:-3])\n","    print(date_prefix, *args, file=self._fd, flush=True)\n","    if self.also_to_stdout:\n","      print(date_prefix, *args, flush=True)\n","\n","  def opt_print(self, *args):\n","    if not self._quiet:\n","      self.print(*args)\n","\n","  def __enter__(self):\n","    return self\n","\n","  def __exit__(self, unused_exception_type, unused_exc_value, unused_traceback):\n","    self.close()\n","\n","  def close(self):\n","    if self._fd:\n","      self._fd.close()\n","      self._fd = None\n","\n","  def __del__(self):\n","    self.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YtRIC9vU1Rz5"},"outputs":[],"source":["\"\"\"Log data to a jsonl file.\"\"\"\n","\n","import datetime\n","import json\n","import os\n","import time\n","from typing import Any, Dict, Text\n","\n","from open_spiel.python.utils import gfile\n","\n","\n","class DataLoggerJsonLines:\n","  \"\"\"Log data to a jsonl file.\"\"\"\n","\n","  def __init__(self, path: str, name: str, flush=True):\n","    self._fd = gfile.Open(os.path.join(path, name + \".jsonl\"), \"w\")\n","    self._flush = flush\n","    self._start_time = time.time()\n","\n","  def __del__(self):\n","    self.close()\n","\n","  def close(self):\n","    if hasattr(self, \"_fd\") and self._fd is not None:\n","      self._fd.flush()\n","      self._fd.close()\n","      self._fd = None\n","\n","  def flush(self):\n","    self._fd.flush()\n","\n","  def write(self, data: Dict[Text, Any]):\n","    now = time.time()\n","    data[\"time_abs\"] = now\n","    data[\"time_rel\"] = now - self._start_time\n","    dt_now = datetime.datetime.utcfromtimestamp(now)\n","    data[\"time_str\"] = dt_now.strftime(\"%Y-%m-%d %H:%M:%S.%f +0000\")\n","    self._fd.write(json.dumps(data))\n","    self._fd.write(\"\\n\")\n","    if self._flush:\n","      self.flush()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IjBWzYc0eYKA"},"outputs":[],"source":["# –î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è fn/processes that gives a logger and logs exceptions\n","def watcher(fn):\n","    # Wrap the decorated function\n","    @functools.wraps(fn)\n","    def _watcher(*, config, num=None, **kwargs):\n","        name = fn.__name__\n","        if num is not None:\n","            name += \"-\" + str(num)\n","        with FileLogger(config.path, name, config.quiet) as logger:\n","            print(\"{} started\".format(name))\n","            logger.print(\"{} started\".format(name))\n","            try:\n","                return fn(config=config, logger=logger, **kwargs)\n","            except Exception as e:\n","                logger.print(\"\\n\".join([\n","                    \"\",\n","                    \" Exception caught \".center(60, \"=\"),\n","                    traceback.format_exc(),\n","                    \"=\" * 60,\n","                ]))\n","                print(\"Exception caught in {}: {}\".format(name, e))\n","                raise\n","            finally:\n","                logger.print(\"{} exiting\".format(name))\n","                print(\"{} exiting\".format(name))\n","    return _watcher"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w3s5yxO6eaM-"},"outputs":[],"source":["# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞\n","def _init_bot(config, game, evaluator_, evaluation):\n","    noise = None if evaluation else (config.policy_epsilon, config.policy_alpha)\n","    return MCTSBot(\n","        game,\n","        config.uct_c,\n","        config.max_simulations,\n","        evaluator_,\n","        solve=False,\n","        dirichlet_noise=noise,\n","        child_selection_fn=SearchNode.puct_value,\n","        verbose=False,\n","        dont_return_chance_node=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eNLU-4aVeqvH"},"outputs":[],"source":["# Play one game, return the trajectory\n","def _play_game(logger, game_num, game, bots, temperature, temperature_drop):\n","    trajectory = Trajectory()\n","    actions = []\n","    state = game.new_initial_state()\n","    random_state = np.random.RandomState()\n","    logger.opt_print(\" Starting game {} \".format(game_num).center(60, \"-\"))\n","    logger.opt_print(\"Initial state:\\n{}\".format(state))\n","    while not state.is_terminal():\n","        root = bots[state.current_player()].mcts_search(state)\n","        policy = np.zeros(game.num_distinct_actions())\n","        for c in root.children:\n","            policy[c.action] = c.explore_count\n","        policy = policy**(1 / temperature)\n","        policy /= policy.sum()\n","        if len(actions) >= temperature_drop:\n","            action = root.best_child().action\n","        else:\n","            action = np.random.choice(len(policy), p=policy)\n","        trajectory.states.append(\n","            TrajectoryState(state.observation_tensor(), state.current_player(),\n","                            state.legal_actions_mask(), action, policy,\n","                            root.total_reward / root.explore_count)\n","        )\n","        action_str = state.action_to_string(state.current_player(), action)\n","        actions.append(action_str)\n","        logger.opt_print(\"Player {} sampled action: {}\".format(state.current_player(), action_str))\n","        state.apply_action(action)\n","    logger.opt_print(\"Next state:\\n{}\".format(state))\n","\n","    trajectory.returns = state.returns()\n","    logger.print(\"Game {}: Returns: {}; Actions: {}\".format(\n","        game_num, \" \".join(map(str, trajectory.returns)), \" \".join(actions)))\n","    return trajectory"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BllzS2EFesod"},"outputs":[],"source":["# Read the queue for a checkpoint to load, or an exit signal\n","def update_checkpoint(logger, queue, model, az_evaluator):\n","    path = None\n","    while True:  # Get the last message, ignore intermediate ones.\n","        try:\n","            path = queue.get_nowait()\n","        except spawn.Empty:\n","            break\n","    if path:\n","        logger.print(\"Inference cache:\", az_evaluator.cache_info())\n","        logger.print(\"Loading checkpoint\", path)\n","        model.load_checkpoint(path)\n","        az_evaluator.clear_cache()\n","    elif path is not None:  # Empty string means stop this process.\n","        return False\n","    return True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bktunh3iet43"},"outputs":[],"source":["# An actor process runner that generates games and returns trajectories\n","@watcher\n","def actor(*, config, game, logger, queue):\n","    logger.print(\"Initializing model\")\n","    model = _init_model_from_config(config)\n","\n","    logger.print(\"Initializing bots\")\n","    az_evaluator = AlphaZeroEvaluator(game, model)\n","\n","    bots = [_init_bot(config, game, az_evaluator, False),\n","            _init_bot(config, game, az_evaluator, False)]\n","    for game_num in itertools.count():\n","        if not update_checkpoint(logger, queue, model, az_evaluator):\n","            return\n","        queue.put(_play_game(logger, game_num, game, bots, config.temperature, config.temperature_drop))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"77Fk3b0xSZv5"},"outputs":[],"source":["# A process that plays the latest checkpoint vs standard MCTS\n","@watcher\n","def evaluator(*, game, config, logger, queue):\n","    results = Buffer(config.evaluation_window)\n","    logger.print(\"Initializing model\")\n","\n","    model = _init_model_from_config(config)\n","    logger.print(\"Initializing bots\")\n","\n","    az_evaluator = AlphaZeroEvaluator(game, model)\n","    random_evaluator = RandomRolloutEvaluator()\n","\n","    for game_num in itertools.count():\n","        if not update_checkpoint(logger, queue, model, az_evaluator):\n","            return\n","\n","        az_player = game_num % 2\n","        difficulty = (game_num // 2) % config.eval_levels\n","        max_simulations = int(config.max_simulations * (10 ** (difficulty / 2)))\n","        bots = [\n","            _init_bot(config, game, az_evaluator, True),\n","            MCTSBot(\n","                game,\n","                config.uct_c,\n","                max_simulations,\n","                random_evaluator,\n","                solve=True,\n","                verbose=False,\n","                dont_return_chance_node=True)\n","        ]\n","        if az_player == 1:\n","            bots = list(reversed(bots))\n","\n","        trajectory = _play_game(logger, game_num, game, bots, temperature=1, temperature_drop=0)\n","        results.append(trajectory.returns[az_player])\n","        queue.put((difficulty, trajectory.returns[az_player]))\n","\n","        logger.print(\"AZ: {}, MCTS: {}, AZ avg/{}: {:.3f}\".format(\n","            trajectory.returns[az_player],\n","            trajectory.returns[1 - az_player],\n","            len(results), np.mean(results.data))\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b263MHYEenkG"},"outputs":[],"source":["# Learner, –≤—ã–ø–æ–ª–Ω—è—é—â–∏–π –æ–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –Ω–∞ –æ–ø—ã—Ç–µ –∏–∑ replay buffer'–∞\n","@watcher\n","def learner(*, game, config, actors, evaluators, broadcast_fn, logger):\n","    logger.also_to_stdout = True\n","    replay_buffer = Buffer(config.replay_buffer_size)\n","    learn_rate = config.replay_buffer_size // config.replay_buffer_reuse\n","    logger.print(\"Initializing model\")\n","    model = _init_model_from_config(config)\n","    logger.print(\"Model type: %s(%s, %s)\" % (config.nn_model, config.nn_width, config.nn_depth))\n","    logger.print(\"Model size:\", model.num_trainable_variables, \"variables\")\n","    save_path = model.save_checkpoint(0)\n","    logger.print(\"Initial checkpoint:\", save_path)\n","    broadcast_fn(save_path)\n","\n","    data_log = DataLoggerJsonLines(config.path, \"learner\", True)\n","\n","    stage_count = 7\n","    value_accuracies = [stats.BasicStats() for _ in range(stage_count)]\n","    value_predictions = [stats.BasicStats() for _ in range(stage_count)]\n","    game_lengths = stats.BasicStats()\n","    game_lengths_hist = stats.HistogramNumbered(game.max_game_length() + 1)\n","    outcomes = stats.HistogramNamed([\"Player1\", \"Player2\", \"Draw\"])\n","    evals = [Buffer(config.evaluation_window) for _ in range(config.eval_levels)]\n","    total_trajectories = 0\n","\n","\n","    # Merge all the actor queues into a single generator\n","    def trajectory_generator():\n","        while True:\n","          found = 0\n","          for actor_process in actors:\n","              try:\n","                  yield actor_process.queue.get_nowait()\n","              except spawn.Empty:\n","                  pass\n","              else:\n","                  found += 1\n","          if found == 0:\n","              time.sleep(0.01)  # 10ms\n","\n","    # Collects the trajectories from actors into the replay buffer\n","    def collect_trajectories():\n","      num_trajectories = 0\n","      num_states = 0\n","      for trajectory in trajectory_generator():\n","          num_trajectories += 1\n","          num_states += len(trajectory.states)\n","          game_lengths.add(len(trajectory.states))\n","          game_lengths_hist.add(len(trajectory.states))\n","\n","          p1_outcome = trajectory.returns[0]\n","          if p1_outcome > 0:\n","              outcomes.add(0)\n","          elif p1_outcome < 0:\n","              outcomes.add(1)\n","          else:\n","              outcomes.add(2)\n","\n","          replay_buffer.extend(\n","              model_lib.TrainInput(\n","                  s.observation, s.legals_mask, s.policy, p1_outcome)\n","              for s in trajectory.states)\n","\n","          for stage in range(stage_count):\n","              # Scale for the length of the game\n","              index = (len(trajectory.states) - 1) * stage // (stage_count - 1)\n","              n = trajectory.states[index]\n","              accurate = (n.value >= 0) == (trajectory.returns[n.current_player] >= 0)\n","              value_accuracies[stage].add(1 if accurate else 0)\n","              value_predictions[stage].add(abs(n.value))\n","\n","          if num_states >= learn_rate:\n","              break\n","      return num_trajectories, num_states\n","\n","\n","    # Sample from the replay buffer, update weights and save a checkpoint\n","    def learn(step):\n","        losses = []\n","        for _ in range(len(replay_buffer) // config.train_batch_size):\n","            data = replay_buffer.sample(config.train_batch_size)\n","            losses.append(model.update(data))\n","\n","        # Always save a checkpoint, either for keeping or for loading the weights to\n","        # the actors. It only allows numbers, so use -1 as \"latest\".\n","        save_path = model.save_checkpoint(\n","            step if step % config.checkpoint_freq == 0 else -1)\n","        losses = sum(losses, model_lib.Losses(0, 0, 0)) / len(losses)\n","        logger.print(losses)\n","        logger.print(\"Checkpoint saved:\", save_path)\n","        return save_path, losses\n","\n","    last_time = time.time() - 60\n","    for step in itertools.count(1):\n","        for value_accuracy in value_accuracies:\n","            value_accuracy.reset()\n","        for value_prediction in value_predictions:\n","            value_prediction.reset()\n","        game_lengths.reset()\n","        game_lengths_hist.reset()\n","        outcomes.reset()\n","\n","        num_trajectories, num_states = collect_trajectories()\n","        total_trajectories += num_trajectories\n","        now = time.time()\n","        seconds = now - last_time\n","        last_time = now\n","\n","        logger.print(\"Step:\", step)\n","        logger.print(\n","            (\"Collected {:5} states from {:3} games, {:.1f} states/s. \"\n","            \"{:.1f} states/(s*actor), game length: {:.1f}\").format(\n","                num_states, num_trajectories, num_states / seconds,\n","                num_states / (config.actors * seconds),\n","                num_states / num_trajectories))\n","        logger.print(\"Buffer size: {}. States seen: {}\".format(\n","            len(replay_buffer), replay_buffer.total_seen))\n","\n","        save_path, losses = learn(step)\n","\n","        for eval_process in evaluators:\n","            while True:\n","                try:\n","                    difficulty, outcome = eval_process.queue.get_nowait()\n","                    evals[difficulty].append(outcome)\n","                except spawn.Empty:\n","                    break\n","\n","        batch_size_stats = stats.BasicStats()  # Only makes sense in C++.\n","        batch_size_stats.add(1)\n","        data_log.write({\n","            \"step\": step,\n","            \"total_states\": replay_buffer.total_seen,\n","            \"states_per_s\": num_states / seconds,\n","            \"states_per_s_actor\": num_states / (config.actors * seconds),\n","            \"total_trajectories\": total_trajectories,\n","            \"trajectories_per_s\": num_trajectories / seconds,\n","            \"queue_size\": 0,  # Only available in C++.\n","            \"game_length\": game_lengths.as_dict,\n","            \"game_length_hist\": game_lengths_hist.data,\n","            \"outcomes\": outcomes.data,\n","            \"value_accuracy\": [v.as_dict for v in value_accuracies],\n","            \"value_prediction\": [v.as_dict for v in value_predictions],\n","            \"eval\": {\n","                \"count\": evals[0].total_seen,\n","                \"results\": [sum(e.data) / len(e) if e else 0 for e in evals],\n","            },\n","            \"batch_size\": batch_size_stats.as_dict,\n","            \"batch_size_hist\": [0, 1],\n","            \"loss\": {\n","                \"policy\": losses.policy,\n","                \"value\": losses.value,\n","                \"l2reg\": losses.l2,\n","                \"sum\": losses.total,\n","            },\n","            \"cache\": {  # Null stats because it's hard to report between processes.\n","                \"size\": 0,\n","                \"max_size\": 0,\n","                \"usage\": 0,\n","                \"requests\": 0,\n","                \"requests_per_s\": 0,\n","                \"hits\": 0,\n","                \"misses\": 0,\n","                \"misses_per_s\": 0,\n","                \"hit_rate\": 0,\n","            },\n","        })\n","        logger.print()\n","\n","        if config.max_steps > 0 and step >= config.max_steps:\n","            break\n","\n","        broadcast_fn(save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SbVUA0u5ekeA"},"outputs":[],"source":["JOIN_WAIT_DELAY = 0.001\n","\n","# Start all the worker processes for a full alphazero setup\n","def alpha_zero(config: Config):\n","    game = pyspiel.load_game(config.game)\n","    config = config._replace(\n","        observation_shape=game.observation_tensor_shape(),\n","        output_size=game.num_distinct_actions()\n","    )\n","\n","    path = config.path\n","    if not path:\n","        path = tempfile.mkdtemp(prefix=\"az-{}-{}-\".format(datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\"), config.game))\n","        config = config._replace(path=path)\n","\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","    if not os.path.isdir(path):\n","        sys.exit(\"{} isn't a directory\".format(path))\n","    print(\"Writing logs and checkpoints to:\", path)\n","    print(\"Model type: %s(%s, %s)\" % (config.nn_model, config.nn_width, config.nn_depth))\n","\n","    with open(os.path.join(config.path, \"config.json\"), \"w\") as fp:\n","        fp.write(json.dumps(config._asdict(), indent=2, sort_keys=True) + \"\\n\")\n","\n","    actors = [spawn.Process(actor, kwargs={\"game\": game, \"config\": config, \"num\": i})\n","              for i in range(config.actors)]\n","    evaluators = [spawn.Process(evaluator, kwargs={\"game\": game, \"config\": config, \"num\": i})\n","                  for i in range(config.evaluators)]\n","\n","    def broadcast(msg):\n","        for proc in actors + evaluators:\n","          proc.queue.put(msg)\n","\n","    try:\n","        learner(game=game, config=config, actors=actors,  # pylint: disable=missing-kwoa\n","                evaluators=evaluators, broadcast_fn=broadcast)\n","    except (KeyboardInterrupt, EOFError):\n","        print(\"Caught a KeyboardInterrupt, stopping early.\")\n","    finally:\n","        broadcast(\"\")\n","        # for actor processes to join we have to make sure that their q_in is empty, including backed up items\n","        for proc in actors:\n","            while proc.exitcode is None:\n","                while not proc.queue.empty():\n","                    proc.queue.get_nowait()\n","                proc.join(JOIN_WAIT_DELAY)\n","        for proc in evaluators:\n","            proc.join()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C42hCTFurb6K"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pZ4lyIrL4KS3"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["4OvXRohFxmhu","LeHJdQdPgXJn","5ETKq1oIvaza","Sh9y5m1egpOi"],"provenance":[],"mount_file_id":"1mAfsjRajj8IMHQ2Eh9YRiX0xVFIER9xz","authorship_tag":"ABX9TyN7L0Ffa0dMKqJzqEwae+Qg"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}